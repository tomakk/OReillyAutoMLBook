{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Chapter 10: Computer Vision with AutoGluon - Complete Implementation Notebook\n",
    "## Retail Product Classification and Multimodal E-commerce Applications\n",
    "\n",
    "This comprehensive notebook provides detailed implementations for all concepts covered in Chapter 10, focusing on **retail and e-commerce computer vision applications**. It includes complete code examples, advanced techniques, and production-ready implementations using the **Mini Fashion Product Images and Text Dataset** from Kaggle.\n",
    "\n",
    "All code examples are tested and validated with **AutoGluon 1.5.0**, featuring the latest computer vision improvements including enhanced object detection presets with ~20% relative improvements in mean Average Precision (mAP) metrics, PDF document classification, and Open Vocabulary Object Detection.\n",
    "\n",
    "### Contents\n",
    "1. Environment Setup and Dataset Download\n",
    "2. Fashion Product Image Classification\n",
    "3. Multimodal Product Classification (Images + Text)\n",
    "4. Model Architecture Deep Dive (CNNs vs ViTs)\n",
    "5. Object Detection for Retail\n",
    "6. Handling Missing Modalities in Production\n",
    "7. Model Evaluation and A/B Testing\n",
    "8. Production Deployment and Monitoring\n",
    "9. Performance Optimization for Resource-Constrained Environments\n",
    "10. Summary and Best Practices\n",
    "\n",
    "### Dataset: Mini Fashion Product Images and Text Dataset\n",
    "\n",
    "**Source**: [Kaggle - Mini Product Image and Text Dataset](https://www.kaggle.com/datasets/nirmalsankalana/mini-product-image-and-text-dataset)\n",
    "\n",
    "This dataset contains:\n",
    "- Fashion product images in multiple categories\n",
    "- Product titles and descriptions\n",
    "- Category labels for classification\n",
    "- Perfect for demonstrating AutoGluon's multimodal capabilities in retail scenarios\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Data Preparation\n\nLet's start by setting up our environment and downloading the mini fashion product images and text dataset we'll be using throughout this notebook.\n\n**[Snippet 10-1]** - AutoGluon installation for computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a01a5f65",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "Install all dependencies from the book's requirements file:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "For Kaggle dataset access, you'll also need to configure your Kaggle API credentials (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries for retail computer vision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# AutoGluon imports\n",
    "from autogluon.multimodal import MultiModalPredictor\n",
    "import autogluon.core as ag\n",
    "\n",
    "# Image processing and visualization\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "\n",
    "# Retail-specific utilities\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set up plotting for retail visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")  # Professional color scheme for retail\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"AutoGluon version: {ag.__version__}\")\n",
    "print(\"Retail computer vision environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5a",
   "metadata": {},
   "source": [
    "### GPU Setup and Hardware Recommendations\n\nComputer vision tasks are computationally intensive. While AutoGluon can work on CPU-only systems, GPU acceleration significantly improves training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setup check optimized for retail applications\n",
    "import torch\n",
    "\n",
    "def check_retail_gpu_setup():\n",
    "    \"\"\"Check GPU configuration for retail computer vision workloads\"\"\"\n",
    "    \n",
    "    print(\"Retail Computer Vision GPU Check:\")\n",
    "    print(f\"   PyTorch version: {torch.__version__}\")\n",
    "    print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU count: {torch.cuda.device_count()}\")\n",
    "        gpu_memory = 0\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "            print(f\"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "            \n",
    "        # Retail-specific memory recommendations\n",
    "        if gpu_memory < 6:\n",
    "            print(\"\\n   Limited GPU memory: Use 'medium_quality' for product classification\")\n",
    "            recommended_preset = 'medium_quality'\n",
    "            recommended_batch_size = 8\n",
    "        elif gpu_memory >= 16:\n",
    "            print(\"\\n   High-end GPU: Perfect for 'best_quality' retail models\")\n",
    "            recommended_preset = 'best_quality'\n",
    "            recommended_batch_size = 32\n",
    "        else:\n",
    "            print(\"\\n   Good GPU memory: 'high_quality' preset recommended\")\n",
    "            recommended_preset = 'high_quality'\n",
    "            recommended_batch_size = 16\n",
    "            \n",
    "        print(f\"   Recommended preset: {recommended_preset}\")\n",
    "        print(f\"   Recommended batch size: {recommended_batch_size}\")\n",
    "        \n",
    "        # Note about production inference\n",
    "        print(\"\\n   Note: For production INFERENCE (not training), hardware requirements\")\n",
    "        print(\"   are much more flexible. Smaller GPUs or even CPUs can work well\")\n",
    "        print(\"   depending on your throughput requirements.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n   CPU-only mode: Suitable for small retail catalogs and inference\")\n",
    "        print(\"   Consider GPU for training on large-scale e-commerce applications\")\n",
    "        recommended_preset = 'medium_quality'\n",
    "        recommended_batch_size = 4\n",
    "        \n",
    "    return recommended_preset, recommended_batch_size\n",
    "\n",
    "RECOMMENDED_PRESET, BATCH_SIZE = check_retail_gpu_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle API Setup\n",
    "To download the Fashion Product Images dataset, you need Kaggle API credentials:\n",
    "1. Create a Kaggle account at https://www.kaggle.com\n",
    "2. Go to Account Settings > API > Create New Token\n",
    "3. Place the downloaded `kaggle.json` in `~/.kaggle/kaggle.json`\n",
    "4. Run `chmod 600 ~/.kaggle/kaggle.json`\n",
    "\n",
    "If you don't have Kaggle credentials, the notebook will use a fallback synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Fashion Product Dataset from Kaggle\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"data.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"nirmalsankalana/mini-product-image-and-text-dataset\",\n",
    "    file_path,\n",
    ")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 records:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore and prepare the product dataset\n",
    "def explore_product_dataset(df):\n",
    "    \"\"\"\n",
    "    Explore the structure and contents of the product dataset\n",
    "    \"\"\"\n",
    "    print(\"Dataset Exploration:\")\n",
    "    print(f\"  Total products: {len(df)}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"\\n  Data types:\")\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        print(f\"    {col}: {dtype}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\nMissing Values:\")\n",
    "    missing_counts = df.isnull().sum()\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {col}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Analyze categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if col not in ['image_path', 'title', 'description', 'image']:  # Skip text columns\n",
    "            unique_count = df[col].nunique()\n",
    "            print(f\"\\n{col}: {unique_count} unique values\")\n",
    "            if unique_count <= 20:  # Show distribution for categorical variables\n",
    "                print(df[col].value_counts().head(10))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Explore the dataset\n",
    "if 'df' in locals() and df is not None:\n",
    "    explore_product_dataset(df)\n",
    "else:\n",
    "    print(\"Please run the dataset download section first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for AutoGluon\n",
    "def prepare_fashion_data(df):\n",
    "    \"\"\"Prepare fashion dataset for AutoGluon computer vision\"\"\"\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"No dataset to prepare\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Preparing dataset for AutoGluon...\")\n",
    "    \n",
    "    # Make a copy to avoid modifying original\n",
    "    prepared_df = df.copy()\n",
    "    \n",
    "    # Identify image and label columns (adjust based on actual dataset structure)\n",
    "    print(\"\\nAnalyzing dataset structure...\")\n",
    "    \n",
    "    # Common column name patterns for images\n",
    "    image_col_patterns = ['image', 'img', 'filename', 'file', 'path']\n",
    "    image_col = None\n",
    "    for col in prepared_df.columns:\n",
    "        if any(pattern in col.lower() for pattern in image_col_patterns):\n",
    "            image_col = col\n",
    "            break\n",
    "    \n",
    "    # Common column name patterns for labels/categories\n",
    "    label_col_patterns = ['category', 'class', 'label', 'type']\n",
    "    label_col = None\n",
    "    for col in prepared_df.columns:\n",
    "        if any(pattern in col.lower() for pattern in label_col_patterns):\n",
    "            label_col = col\n",
    "            break\n",
    "    \n",
    "    print(f\"Image column: {image_col}\")\n",
    "    print(f\"Label column: {label_col}\")\n",
    "    \n",
    "    if not image_col or not label_col:\n",
    "        print(\"\\nCould not automatically identify image and label columns.\")\n",
    "        print(\"Available columns:\", list(prepared_df.columns))\n",
    "        return None\n",
    "    \n",
    "    # Standardize column names for AutoGluon\n",
    "    if image_col != 'image':\n",
    "        prepared_df = prepared_df.rename(columns={image_col: 'image'})\n",
    "    if label_col != 'label':\n",
    "        prepared_df = prepared_df.rename(columns={label_col: 'label'})\n",
    "    \n",
    "    # Display label distribution\n",
    "    if 'label' in prepared_df.columns:\n",
    "        print(\"\\nLabel Distribution:\")\n",
    "        print(prepared_df['label'].value_counts())\n",
    "    \n",
    "    print(f\"\\nPrepared dataset with {len(prepared_df)} products\")\n",
    "    return prepared_df\n",
    "\n",
    "# Prepare the dataset\n",
    "if 'df' in locals() and df is not None:\n",
    "    prepared_fashion_df = prepare_fashion_data(df)\n",
    "else:\n",
    "    print(\"Dataset not ready for preparation\")\n",
    "    prepared_fashion_df = None"
   ]
  },
  {
   "cell_type": "code",
   "id": "qhs5gkkcw6h",
   "source": "# Figure 10-1: AutoGluon Computer Vision Workflow Diagram\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom matplotlib.patches import FancyBboxPatch\n\ndef create_workflow_diagram():\n    \"\"\"Create Figure 10-1: AutoGluon CV Workflow\"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize=(14, 4))\n    ax.set_xlim(0, 14)\n    ax.set_ylim(0, 4)\n    ax.axis('off')\n    \n    # Define workflow stages\n    boxes = [\n        {'x': 0.5, 'label': 'Raw\\nImages', 'color': '#E8F4FD'},\n        {'x': 3.0, 'label': 'Data\\nPreprocessing', 'color': '#FFF3E0'},\n        {'x': 5.5, 'label': 'Model\\nTraining', 'color': '#E8F5E9'},\n        {'x': 8.0, 'label': 'Evaluation', 'color': '#F3E5F5'},\n        {'x': 10.5, 'label': 'Deployment', 'color': '#FFEBEE'},\n    ]\n    \n    # Draw boxes and arrows\n    for i, box in enumerate(boxes):\n        rect = FancyBboxPatch((box['x'], 1.2), 2, 1.6, \n                              boxstyle=\"round,pad=0.05,rounding_size=0.2\",\n                              facecolor=box['color'], edgecolor='#333333', linewidth=2)\n        ax.add_patch(rect)\n        ax.text(box['x'] + 1, 2, box['label'], ha='center', va='center', \n                fontsize=12, fontweight='bold', color='#333333')\n        \n        if i < len(boxes) - 1:\n            ax.annotate('', xy=(boxes[i+1]['x'] - 0.1, 2), xytext=(box['x'] + 2.1, 2),\n                       arrowprops=dict(arrowstyle='->', color='#666666', lw=2))\n    \n    ax.text(7, 3.6, 'Figure 10-1: AutoGluon Computer Vision Workflow', \n            ha='center', va='center', fontsize=16, fontweight='bold', color='#1a1a1a')\n    \n    plt.tight_layout()\n    plt.savefig('figure_10_1_workflow.png', dpi=150, bbox_inches='tight', \n                facecolor='white', edgecolor='none')\n    plt.show()\n    print(\"Figure 10-1 saved as 'figure_10_1_workflow.png'\")\n\ncreate_workflow_diagram()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Classification with MultiModalPredictor\n\nImage classification is the foundation of most computer vision applications. We'll build a practical system for classifying different types of products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your first fashion product classifier \n",
    "def train_fashion_classifier(df, preset='medium_quality', time_limit=1800):\n",
    "    \"\"\"Train a fashion product image classifier using AutoGluon 1.5.0\"\"\"\n",
    "    \n",
    "    if df is None or len(df) == 0:\n",
    "        print(\"No data available for training\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Training fashion product classifier with AutoGluon 1.5.0\")\n",
    "    print(f\"Dataset size: {len(df)} products\")\n",
    "    print(f\"Preset: {preset}\")\n",
    "    print(f\"Time limit: {time_limit // 60} minutes\")\n",
    "    \n",
    "    # Ensure we have the required columns\n",
    "    if 'image' not in df.columns or 'label' not in df.columns:\n",
    "        print(\"Dataset must have 'image' and 'label' columns\")\n",
    "        return None\n",
    "    \n",
    "    # Check class distribution and filter out classes with too few samples\n",
    "    print(\"\\nAnalyzing class distribution...\")\n",
    "    class_counts = df['label'].value_counts()\n",
    "    print(\"Class distribution:\")\n",
    "    for label, count in class_counts.items():\n",
    "        print(f\"   {label}: {count} samples\")\n",
    "    \n",
    "    # Filter out classes with fewer than 2 samples for stratified splitting\n",
    "    min_samples_per_class = 2\n",
    "    valid_classes = class_counts[class_counts >= min_samples_per_class].index\n",
    "    \n",
    "    if len(valid_classes) < len(class_counts):\n",
    "        print(f\"\\nFiltering out classes with < {min_samples_per_class} samples\")\n",
    "        df = df[df['label'].isin(valid_classes)].copy()\n",
    "    \n",
    "    # Split data with stratification\n",
    "    # Note: For highly imbalanced categories, consider using class weights or oversampling\n",
    "    print(f\"\\nSplitting data with stratification...\")\n",
    "    \n",
    "    try:\n",
    "        train_df, temp_df = train_test_split(\n",
    "            df, test_size=0.2, random_state=42,\n",
    "            stratify=df['label']\n",
    "        )\n",
    "        val_df, test_df = train_test_split(\n",
    "            temp_df, test_size=0.5, random_state=42,\n",
    "            stratify=temp_df['label'] if len(temp_df) >= len(temp_df['label'].unique()) * 2 else None\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"Stratified split failed, using random split: {e}\")\n",
    "        train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"   Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "    \n",
    "    # Initialize the predictor\n",
    "    print(f\"\\nInitializing MultiModalPredictor...\")\n",
    "    \n",
    "    predictor = MultiModalPredictor(\n",
    "        label='label',\n",
    "        path='./fashion_classifier',\n",
    "        eval_metric='accuracy',\n",
    "        problem_type='classification'\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nStarting training...\")\n",
    "    print(\"AutoGluon automatically handles:\")\n",
    "    print(\"   - Image preprocessing (resizing, normalization)\")\n",
    "    print(\"   - Data augmentation (rotation, flipping, color adjustment, etc.)\")\n",
    "    print(\"   - Model architecture selection\")\n",
    "    print(\"   - Transfer learning from pre-trained models\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        predictor.fit(\n",
    "            train_df,\n",
    "            tuning_data=val_df if len(val_df) >= 3 else None,\n",
    "            time_limit=time_limit,\n",
    "            presets=preset\n",
    "        )\n",
    "        \n",
    "        training_time = datetime.now() - start_time\n",
    "        print(f\"\\nTraining completed in {training_time}\")\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        print(\"\\nEvaluating on test set...\")\n",
    "        test_results = predictor.evaluate(test_df)\n",
    "        \n",
    "        print(\"\\nTest Results:\")\n",
    "        for metric, value in test_results.items():\n",
    "            print(f\"   {metric}: {value:.4f}\")\n",
    "        \n",
    "        return predictor, test_df, test_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Train the fashion classifier\n",
    "if prepared_fashion_df is not None:\n",
    "    sample_size = min(500, len(prepared_fashion_df))\n",
    "    demo_df = prepared_fashion_df.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    print(f\"Training on {sample_size} products for demonstration\")\n",
    "    print(\"For production, use the full dataset\\n\")\n",
    "    \n",
    "    predictor, test_data, results = train_fashion_classifier(\n",
    "        demo_df, \n",
    "        preset=RECOMMENDED_PRESET,\n",
    "        time_limit=1800  # 30 minutes\n",
    "    )\n",
    "else:\n",
    "    print(\"Dataset not ready for training\")\n",
    "    predictor, test_data, results = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "id": "uw71bze197h",
   "source": "# Figure 10-2: AutoGluon Automatic Preprocessing Pipeline\ndef create_preprocessing_diagram():\n    \"\"\"Create Figure 10-2: Preprocessing Pipeline with sample image transformation\"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize=(14, 5))\n    ax.set_xlim(0, 14)\n    ax.set_ylim(0, 5)\n    ax.axis('off')\n    \n    # Define pipeline stages\n    stages = [\n        {'x': 0.3, 'label': 'Input\\nImage', 'color': '#BBDEFB', 'detail': '(Variable Size)'},\n        {'x': 2.8, 'label': 'Resize', 'color': '#C8E6C9', 'detail': '(224×224)'},\n        {'x': 5.3, 'label': 'Normalize', 'color': '#FFE0B2', 'detail': '(ImageNet Stats)'},\n        {'x': 7.8, 'label': 'Augment', 'color': '#E1BEE7', 'detail': '(Random Transforms)'},\n        {'x': 10.3, 'label': 'Model\\nInput', 'color': '#FFCDD2', 'detail': '(Tensor)'},\n    ]\n    \n    # Draw stages\n    for i, stage in enumerate(stages):\n        rect = FancyBboxPatch((stage['x'], 1.5), 2.2, 2, \n                              boxstyle=\"round,pad=0.05,rounding_size=0.2\",\n                              facecolor=stage['color'], edgecolor='#333333', linewidth=2)\n        ax.add_patch(rect)\n        \n        ax.text(stage['x'] + 1.1, 2.7, stage['label'], ha='center', va='center', \n                fontsize=11, fontweight='bold', color='#333333')\n        ax.text(stage['x'] + 1.1, 1.9, stage['detail'], ha='center', va='center', \n                fontsize=9, color='#666666')\n        \n        if i < len(stages) - 1:\n            ax.annotate('', xy=(stages[i+1]['x'] - 0.1, 2.5), xytext=(stage['x'] + 2.3, 2.5),\n                       arrowprops=dict(arrowstyle='->', color='#666666', lw=2))\n    \n    ax.text(7, 4.5, 'Figure 10-2: AutoGluon Automatic Preprocessing Pipeline', \n            ha='center', va='center', fontsize=16, fontweight='bold', color='#1a1a1a')\n    \n    # Add augmentation examples below\n    aug_text = \"Augmentations: Rotation, Flipping, Color Adjustment, Cropping, Scaling, Perspective\"\n    ax.text(7, 0.7, aug_text, ha='center', va='center', fontsize=10, \n            style='italic', color='#555555')\n    \n    plt.tight_layout()\n    plt.savefig('figure_10_2_preprocessing.png', dpi=150, bbox_inches='tight', \n                facecolor='white', edgecolor='none')\n    plt.show()\n    print(\"Figure 10-2 saved as 'figure_10_2_preprocessing.png'\")\n\ncreate_preprocessing_diagram()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "nnfq4kc132",
   "source": [
    "# Figure 10-5: Sample Product Images from the Dataset\n",
    "import kagglehub\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def create_product_category_grid(df, n_categories=6):\n",
    "    \"\"\"\n",
    "    Create Figure 10-5: Display sample product images from different categories\n",
    "    Loads images from the local kagglehub cache directory\n",
    "    \"\"\"\n",
    "    \n",
    "    if df is None or len(df) == 0:\n",
    "        print(\"No dataset available.\")\n",
    "        return\n",
    "    \n",
    "    # Get the dataset path from kagglehub\n",
    "    dataset_path = kagglehub.dataset_download(\"nirmalsankalana/mini-product-image-and-text-dataset\")\n",
    "    images_dir = os.path.join(dataset_path, \"data\")\n",
    "    \n",
    "    print(f\"Dataset path: {dataset_path}\")\n",
    "    print(f\"Images directory: {images_dir}\")\n",
    "    print(f\"Dataset columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Identify columns - the dataset has 'image' for filename and 'category' for labels\n",
    "    image_col = 'image'\n",
    "    label_col = 'category'\n",
    "    \n",
    "    print(f\"Using image column: {image_col}\")\n",
    "    print(f\"Using label column: {label_col}\")\n",
    "    \n",
    "    # Get top categories\n",
    "    categories = df[label_col].value_counts().head(n_categories).index.tolist()\n",
    "    print(f\"\\nTop {n_categories} categories: {categories}\")\n",
    "    \n",
    "    # Create figure\n",
    "    n_cols = 3\n",
    "    n_rows = 2\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Color palette for borders and titles\n",
    "    colors = ['#1976D2', '#7B1FA2', '#388E3C', '#F57C00', '#5D4037', '#D32F2F']\n",
    "    \n",
    "    for idx, category in enumerate(categories[:6]):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Get a sample image for this category\n",
    "        category_df = df[df[label_col] == category]\n",
    "        image_filename = category_df[image_col].iloc[0]\n",
    "        image_path = os.path.join(images_dir, image_filename)\n",
    "        \n",
    "        print(f\"Loading {category}: {image_filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the image from local file\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            ax.imshow(img)\n",
    "            print(f\"  Successfully loaded: {image_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to load: {e}\")\n",
    "            # Show placeholder with category name\n",
    "            ax.set_facecolor(colors[idx % len(colors)] + '20')\n",
    "            ax.text(0.5, 0.5, f'{category}\\n(Image unavailable)', \n",
    "                   ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "                   color=colors[idx % len(colors)], transform=ax.transAxes)\n",
    "        \n",
    "        ax.set_title(category, fontsize=12, fontweight='bold', \n",
    "                    color=colors[idx % len(colors)], pad=10)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor(colors[idx % len(colors)])\n",
    "            spine.set_linewidth(2)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for ax in axes[len(categories):]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    fig.suptitle('Figure 10-5: Sample Product Images by Category', \n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure_10_5_categories.png', dpi=150, bbox_inches='tight', \n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.show()\n",
    "    print(\"\\nFigure 10-5 saved as 'figure_10_5_categories.png'\")\n",
    "\n",
    "# Create Figure 10-5 using the dataset\n",
    "if 'df' in locals() and df is not None:\n",
    "    create_product_category_grid(df)\n",
    "else:\n",
    "    print(\"Dataset not loaded. Please run the data loading cells first.\")\n",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "23u84w1cg65",
   "source": [
    "# Figure 10-7: Data Augmentation Techniques Visualization\n",
    "import kagglehub\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "\n",
    "def create_augmentation_visualization(df=None):\n",
    "    \"\"\"\n",
    "    Create Figure 10-7: Demonstrate data augmentation techniques\n",
    "    Uses actual dataset image from local kagglehub cache\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    augmentations = [\n",
    "        {'name': 'Original', 'transform': None},\n",
    "        {'name': 'Horizontal Flip', 'transform': 'flip'},\n",
    "        {'name': 'Rotation (15deg)', 'transform': 'rotate'},\n",
    "        {'name': 'Brightness +20%', 'transform': 'bright'},\n",
    "        {'name': 'Contrast +30%', 'transform': 'contrast'},\n",
    "        {'name': 'Random Crop', 'transform': 'crop'},\n",
    "        {'name': 'Color Jitter', 'transform': 'color'},\n",
    "        {'name': 'Gaussian Blur', 'transform': 'blur'},\n",
    "    ]\n",
    "    \n",
    "    # Try to load a real image from the dataset\n",
    "    sample_image = None\n",
    "    if df is not None:\n",
    "        try:\n",
    "            # Get the dataset path from kagglehub\n",
    "            dataset_path = kagglehub.dataset_download(\"nirmalsankalana/mini-product-image-and-text-dataset\")\n",
    "            images_dir = os.path.join(dataset_path, \"data\")\n",
    "            \n",
    "            # Get the first image filename\n",
    "            image_filename = df['image'].iloc[0]\n",
    "            image_path = os.path.join(images_dir, image_filename)\n",
    "            \n",
    "            print(f\"Loading image: {image_path}\")\n",
    "            sample_image = Image.open(image_path).convert('RGB')\n",
    "            sample_image = sample_image.resize((224, 224))\n",
    "            print(\"Successfully loaded dataset image!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not load dataset image: {e}\")\n",
    "    \n",
    "    # Create a sample product image if none available\n",
    "    if sample_image is None:\n",
    "        print(\"Creating synthetic sample image...\")\n",
    "        # Create a colorful product-like image\n",
    "        img_array = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        # Add a gradient background\n",
    "        for i in range(224):\n",
    "            for j in range(224):\n",
    "                img_array[i, j] = [200 + i//8, 180 - j//8, 150]\n",
    "        # Add a simple shape to represent a product\n",
    "        img_array[50:174, 62:162] = [60, 60, 120]  # Blue rectangle\n",
    "        img_array[70:154, 82:142] = [200, 200, 220]  # Inner highlight\n",
    "        sample_image = Image.fromarray(img_array)\n",
    "    \n",
    "    # Apply augmentations and display\n",
    "    for idx, aug in enumerate(augmentations):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Create augmented version\n",
    "        if aug['transform'] is None:\n",
    "            aug_img = sample_image.copy()\n",
    "        elif aug['transform'] == 'flip':\n",
    "            aug_img = sample_image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        elif aug['transform'] == 'rotate':\n",
    "            aug_img = sample_image.rotate(15, fillcolor=(255, 255, 255))\n",
    "        elif aug['transform'] == 'bright':\n",
    "            enhancer = ImageEnhance.Brightness(sample_image)\n",
    "            aug_img = enhancer.enhance(1.2)\n",
    "        elif aug['transform'] == 'contrast':\n",
    "            enhancer = ImageEnhance.Contrast(sample_image)\n",
    "            aug_img = enhancer.enhance(1.3)\n",
    "        elif aug['transform'] == 'crop':\n",
    "            # Random crop and resize\n",
    "            width, height = sample_image.size\n",
    "            left = width // 10\n",
    "            top = height // 10\n",
    "            right = width - width // 10\n",
    "            bottom = height - height // 10\n",
    "            aug_img = sample_image.crop((left, top, right, bottom)).resize((224, 224))\n",
    "        elif aug['transform'] == 'color':\n",
    "            enhancer = ImageEnhance.Color(sample_image)\n",
    "            aug_img = enhancer.enhance(1.5)\n",
    "        elif aug['transform'] == 'blur':\n",
    "            aug_img = sample_image.filter(ImageFilter.GaussianBlur(radius=2))\n",
    "        else:\n",
    "            aug_img = sample_image.copy()\n",
    "        \n",
    "        ax.imshow(aug_img)\n",
    "        ax.set_title(aug['name'], fontsize=11, fontweight='bold', pad=8)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # Color the border based on whether it's original or augmented\n",
    "        border_color = '#2E7D32' if aug['transform'] is None else '#1565C0'\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor(border_color)\n",
    "            spine.set_linewidth(2)\n",
    "    \n",
    "    fig.suptitle('Figure 10-7: Common Data Augmentation Techniques for Retail Images', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure_10_7_augmentation.png', dpi=150, bbox_inches='tight', \n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.show()\n",
    "    print(\"\\nFigure 10-7 saved as 'figure_10_7_augmentation.png'\")\n",
    "\n",
    "# Create Figure 10-7\n",
    "if 'df' in locals() and df is not None:\n",
    "    create_augmentation_visualization(df)\n",
    "else:\n",
    "    create_augmentation_visualization(None)\n",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-10a",
   "metadata": {},
   "source": [
    "## 3. Understanding Model Architectures: CNNs vs Vision Transformers\n",
    "\n",
    "AutoGluon automatically selects between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) based on your dataset. Here's what you need to know:\n",
    "\n",
    "**CNNs (ResNet, EfficientNet):**\n",
    "- Excel at capturing spatial patterns and hierarchical features\n",
    "- Work well with smaller datasets\n",
    "- Faster inference\n",
    "\n",
    "**Vision Transformers (ViTs):**\n",
    "- Typically need MORE data than CNNs to perform well\n",
    "- For small datasets (<5,000 images), CNNs like EfficientNet often outperform ViTs\n",
    "- AutoGluon handles this trade-off automatically based on your dataset size\n",
    "\n",
    "**Transfer Learning Note:**\n",
    "- ImageNet pre-training works best for natural images\n",
    "- For specialized domains (medical imaging, satellite imagery, industrial inspection), domain-specific pre-trained models often perform better\n",
    "- AutoGluon provides access to specialized models through TIMM and HuggingFace model hubs"
   ]
  },
  {
   "cell_type": "code",
   "id": "paj5zyz9gl",
   "source": [
    "# Figure 10-6: CNN vs Vision Transformer Architecture Comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Rectangle, FancyBboxPatch\n",
    "import numpy as np\n",
    "\n",
    "def create_cnn_vs_vit_diagram():\n",
    "    \"\"\"\n",
    "    Create Figure 10-6: Compare CNN and Vision Transformer architectures\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # CNN Architecture (left)\n",
    "    ax1 = axes[0]\n",
    "    ax1.set_xlim(0, 10)\n",
    "    ax1.set_ylim(0, 12)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Convolutional Neural Network (CNN)', fontsize=14, fontweight='bold', color='#1565C0', pad=20)\n",
    "    \n",
    "    # Input image\n",
    "    rect = FancyBboxPatch((1, 9), 2, 2, boxstyle=\"round,pad=0.05\", \n",
    "                                    facecolor='#E3F2FD', edgecolor='#1565C0', linewidth=2)\n",
    "    ax1.add_patch(rect)\n",
    "    ax1.text(2, 10, 'Input\\nImage', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Conv layers\n",
    "    colors = ['#BBDEFB', '#90CAF9', '#64B5F6', '#42A5F5']\n",
    "    for i, (y, c) in enumerate(zip([6.5, 5, 3.5, 2], colors)):\n",
    "        rect = FancyBboxPatch((0.5 + i*0.3, y), 3 - i*0.3, 1.2, boxstyle=\"round,pad=0.05\",\n",
    "                                        facecolor=c, edgecolor='#1565C0', linewidth=2)\n",
    "        ax1.add_patch(rect)\n",
    "    \n",
    "    ax1.text(2, 4.5, 'Convolutional\\nLayers', ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Arrow\n",
    "    ax1.annotate('', xy=(2, 6.3), xytext=(2, 8.8), \n",
    "                 arrowprops=dict(arrowstyle='->', color='#1565C0', lw=2))\n",
    "    \n",
    "    # Fully connected\n",
    "    rect = FancyBboxPatch((1, 0.5), 2, 1, boxstyle=\"round,pad=0.05\",\n",
    "                                    facecolor='#1565C0', edgecolor='#0D47A1', linewidth=2)\n",
    "    ax1.add_patch(rect)\n",
    "    ax1.text(2, 1, 'Classification', ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "    \n",
    "    ax1.annotate('', xy=(2, 0.7), xytext=(2, 1.8),\n",
    "                 arrowprops=dict(arrowstyle='->', color='#1565C0', lw=2))\n",
    "    \n",
    "    # Features text\n",
    "    ax1.text(6, 10, 'Key Characteristics:', fontsize=11, fontweight='bold')\n",
    "    ax1.text(6, 9.2, '• Local feature detection', fontsize=10)\n",
    "    ax1.text(6, 8.4, '• Hierarchical learning', fontsize=10)\n",
    "    ax1.text(6, 7.6, '• Translation invariance', fontsize=10)\n",
    "    ax1.text(6, 6.8, '• Works well with less data', fontsize=10)\n",
    "    ax1.text(6, 6.0, '• Efficient for images', fontsize=10)\n",
    "    \n",
    "    # ViT Architecture (right)\n",
    "    ax2 = axes[1]\n",
    "    ax2.set_xlim(0, 10)\n",
    "    ax2.set_ylim(0, 12)\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Vision Transformer (ViT)', fontsize=14, fontweight='bold', color='#7B1FA2', pad=20)\n",
    "    \n",
    "    # Input image with patches\n",
    "    rect = FancyBboxPatch((1, 9), 2, 2, boxstyle=\"round,pad=0.05\",\n",
    "                                    facecolor='#F3E5F5', edgecolor='#7B1FA2', linewidth=2)\n",
    "    ax2.add_patch(rect)\n",
    "    # Draw grid lines for patches\n",
    "    for i in range(3):\n",
    "        ax2.plot([1 + i*0.67, 1 + i*0.67], [9, 11], color='#7B1FA2', lw=1, alpha=0.5)\n",
    "        ax2.plot([1, 3], [9 + i*0.67, 9 + i*0.67], color='#7B1FA2', lw=1, alpha=0.5)\n",
    "    ax2.text(2, 10, 'Image\\nPatches', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Patch embedding\n",
    "    rect = FancyBboxPatch((0.5, 6.5), 3, 1.5, boxstyle=\"round,pad=0.05\",\n",
    "                                    facecolor='#E1BEE7', edgecolor='#7B1FA2', linewidth=2)\n",
    "    ax2.add_patch(rect)\n",
    "    ax2.text(2, 7.25, 'Patch Embeddings\\n+ Position', ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax2.annotate('', xy=(2, 6.3), xytext=(2, 8.8),\n",
    "                 arrowprops=dict(arrowstyle='->', color='#7B1FA2', lw=2))\n",
    "    \n",
    "    # Transformer blocks\n",
    "    rect = FancyBboxPatch((0.5, 3), 3, 3, boxstyle=\"round,pad=0.05\",\n",
    "                                    facecolor='#CE93D8', edgecolor='#7B1FA2', linewidth=2)\n",
    "    ax2.add_patch(rect)\n",
    "    ax2.text(2, 4.5, 'Transformer\\nEncoder\\n(Self-Attention)', ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax2.annotate('', xy=(2, 3.2), xytext=(2, 6.3),\n",
    "                 arrowprops=dict(arrowstyle='->', color='#7B1FA2', lw=2))\n",
    "    \n",
    "    # Classification\n",
    "    rect = FancyBboxPatch((1, 0.5), 2, 1, boxstyle=\"round,pad=0.05\",\n",
    "                                    facecolor='#7B1FA2', edgecolor='#4A148C', linewidth=2)\n",
    "    ax2.add_patch(rect)\n",
    "    ax2.text(2, 1, 'Classification', ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "    \n",
    "    ax2.annotate('', xy=(2, 0.7), xytext=(2, 2.8),\n",
    "                 arrowprops=dict(arrowstyle='->', color='#7B1FA2', lw=2))\n",
    "    \n",
    "    # Features text\n",
    "    ax2.text(6, 10, 'Key Characteristics:', fontsize=11, fontweight='bold')\n",
    "    ax2.text(6, 9.2, '• Global attention mechanism', fontsize=10)\n",
    "    ax2.text(6, 8.4, '• Captures long-range dependencies', fontsize=10)\n",
    "    ax2.text(6, 7.6, '• Requires more training data', fontsize=10)\n",
    "    ax2.text(6, 6.8, '• State-of-the-art performance', fontsize=10)\n",
    "    ax2.text(6, 6.0, '• Flexible architecture', fontsize=10)\n",
    "    \n",
    "    fig.suptitle('Figure 10-6: CNN vs Vision Transformer Architecture Comparison', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figure_10_6_cnn_vit.png', dpi=150, bbox_inches='tight', \n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.show()\n",
    "    print(\"Figure 10-6 saved as 'figure_10_6_cnn_vit.png'\")\n",
    "\n",
    "create_cnn_vs_vit_diagram()\n",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance with detailed metrics\n",
    "def analyze_model_performance(predictor, test_data, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive analysis for MultiModalPredictor models\"\"\"\n",
    "    \n",
    "    if predictor is None or test_data is None:\n",
    "        print(f\"Cannot analyze {model_name} - missing predictor or test data\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Comprehensive {model_name} Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Get predictions and probabilities\n",
    "        predictions = predictor.predict(test_data)\n",
    "        probabilities = predictor.predict_proba(test_data)\n",
    "        true_labels = test_data['label'].values\n",
    "        \n",
    "        # Basic accuracy\n",
    "        accuracy = np.mean(predictions == true_labels)\n",
    "        print(f\"\\nBasic Metrics:\")\n",
    "        print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"   Total Predictions: {len(predictions)}\")\n",
    "        \n",
    "        # Confidence analysis\n",
    "        confidence_scores = probabilities.max(axis=1).values\n",
    "        \n",
    "        print(f\"\\nConfidence Analysis:\")\n",
    "        print(f\"   Average Confidence: {np.mean(confidence_scores):.3f}\")\n",
    "        print(f\"   Median Confidence: {np.median(confidence_scores):.3f}\")\n",
    "        print(f\"   Min/Max: {np.min(confidence_scores):.3f} / {np.max(confidence_scores):.3f}\")\n",
    "        \n",
    "        # Confidence distribution\n",
    "        high_conf = np.mean(confidence_scores >= 0.8) * 100\n",
    "        medium_conf = np.mean((confidence_scores >= 0.6) & (confidence_scores < 0.8)) * 100\n",
    "        low_conf = np.mean(confidence_scores < 0.6) * 100\n",
    "        \n",
    "        print(f\"\\nConfidence Distribution:\")\n",
    "        print(f\"   High (>=0.8): {high_conf:.1f}%\")\n",
    "        print(f\"   Medium (0.6-0.8): {medium_conf:.1f}%\")\n",
    "        print(f\"   Low (<0.6): {low_conf:.1f}%\")\n",
    "        \n",
    "        # Per-class accuracy\n",
    "        unique_labels = sorted(set(true_labels))\n",
    "        print(f\"\\nPer-Class Performance:\")\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            label_mask = true_labels == label\n",
    "            if np.sum(label_mask) > 0:\n",
    "                label_accuracy = np.mean(predictions[label_mask] == true_labels[label_mask])\n",
    "                label_count = np.sum(label_mask)\n",
    "                print(f\"   {label}: {label_accuracy:.3f} ({label_count} samples)\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'confidence_scores': confidence_scores,\n",
    "            'predictions': predictions,\n",
    "            'true_labels': true_labels\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run analysis\n",
    "if predictor is not None and test_data is not None:\n",
    "    image_analysis = analyze_model_performance(predictor, test_data, \"Image Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Object Detection with AutoGluon\n\nObject detection extends beyond classification to identify and locate multiple objects within images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object detection data format example\n",
    "print(\"Object Detection COCO Format Example\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Example COCO format structure\n",
    "coco_example = {\n",
    "    \"images\": [\n",
    "        {\"id\": 1, \"file_name\": \"product_shelf_001.jpg\", \"width\": 800, \"height\": 600}\n",
    "    ],\n",
    "    \"annotations\": [\n",
    "        {\n",
    "            \"id\": 1, \n",
    "            \"image_id\": 1, \n",
    "            \"category_id\": 1,\n",
    "            \"bbox\": [100, 150, 200, 180],  # [x_min, y_min, width, height]\n",
    "            \"area\": 36000, \n",
    "            \"iscrowd\": 0\n",
    "        }\n",
    "    ],\n",
    "    \"categories\": [\n",
    "        {\"id\": 1, \"name\": \"soda_bottle\"},\n",
    "        {\"id\": 2, \"name\": \"cereal_box\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nCOCO Format Structure:\")\n",
    "print(json.dumps(coco_example, indent=2))\n",
    "\n",
    "print(\"\\n\\nAnnotation Tools for Creating COCO Data:\")\n",
    "print(\"   - LabelImg: https://github.com/heartexlabs/labelImg (free, open-source)\")\n",
    "print(\"   - CVAT: https://cvat.ai/ (web-based, free)\")\n",
    "print(\"   - Roboflow: https://roboflow.com/ (cloud-based with free tier)\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "e7nvy94g1mo",
   "source": "# Figure 10-3: Object Detection Output with Bounding Boxes\nfrom matplotlib.patches import Rectangle\n\ndef create_object_detection_figure():\n    \"\"\"Create Figure 10-3: Object Detection Output visualization\"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n    \n    # Create a sample \"retail shelf\" background\n    np.random.seed(42)\n    background = np.ones((100, 100, 3)) * 0.9  # Light gray background\n    \n    # Add some texture variation to simulate a shelf\n    for i in range(0, 100, 20):\n        background[i:i+2, :, :] = 0.7  # Shelf lines\n    \n    ax.imshow(background, extent=[0, 100, 0, 100])\n    \n    # Simulated product detections on a retail shelf\n    detections = [\n        {'bbox': [5, 60, 20, 35], 'label': 'Soda Bottle', 'conf': 0.95, 'color': '#FF5722'},\n        {'bbox': [28, 62, 18, 33], 'label': 'Cereal Box', 'conf': 0.91, 'color': '#4CAF50'},\n        {'bbox': [50, 58, 22, 37], 'label': 'Snack Bag', 'conf': 0.88, 'color': '#2196F3'},\n        {'bbox': [75, 60, 20, 35], 'label': 'Can', 'conf': 0.93, 'color': '#9C27B0'},\n        {'bbox': [8, 15, 25, 30], 'label': 'Detergent', 'conf': 0.87, 'color': '#FF9800'},\n        {'bbox': [45, 12, 28, 33], 'label': 'Water Bottle', 'conf': 0.92, 'color': '#00BCD4'},\n    ]\n    \n    for det in detections:\n        x, y, w, h = det['bbox']\n        \n        # Draw bounding box\n        rect = Rectangle((x, y), w, h, linewidth=3, edgecolor=det['color'], \n                         facecolor=det['color'], alpha=0.2, linestyle='-')\n        ax.add_patch(rect)\n        \n        # Draw border\n        rect_border = Rectangle((x, y), w, h, linewidth=3, edgecolor=det['color'], \n                                facecolor='none', linestyle='-')\n        ax.add_patch(rect_border)\n        \n        # Add label with confidence score\n        label_text = f\"{det['label']}: {det['conf']:.0%}\"\n        ax.text(x, y + h + 2, label_text, fontsize=9, fontweight='bold',\n                color='white', bbox=dict(boxstyle='round,pad=0.3', \n                facecolor=det['color'], edgecolor='none', alpha=0.9))\n    \n    ax.set_xlim(0, 100)\n    ax.set_ylim(0, 100)\n    ax.set_title('Figure 10-3: Object Detection Output with Bounding Boxes\\n(Retail Shelf Example)', \n                 fontsize=14, fontweight='bold', pad=15)\n    ax.set_xlabel('Products detected with confidence scores', fontsize=10)\n    ax.axis('off')\n    \n    # Add legend\n    legend_text = \"mAP@0.5: Lenient threshold | mAP@0.5:0.95: COCO standard (stricter)\"\n    ax.text(50, -5, legend_text, ha='center', fontsize=9, style='italic', color='#666666')\n    \n    plt.tight_layout()\n    plt.savefig('figure_10_3_detection.png', dpi=150, bbox_inches='tight', \n                facecolor='white', edgecolor='none')\n    plt.show()\n    print(\"Figure 10-3 saved as 'figure_10_3_detection.png'\")\n\ncreate_object_detection_figure()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multimodal Computer Vision Applications\n\nOne of AutoGluon's most powerful capabilities is seamlessly combining image data with other modalities like text descriptions and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare multimodal dataset (images + text)\n",
    "def prepare_multimodal_data(df):\n",
    "    \"\"\"Prepare dataset for multimodal learning (images + text)\"\"\"\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"No data available\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Preparing multimodal dataset (images + text)...\")\n",
    "    \n",
    "    # Create a copy for multimodal preparation\n",
    "    multimodal_df = df.copy()\n",
    "    \n",
    "    # Identify text columns\n",
    "    text_columns = []\n",
    "    for col in multimodal_df.columns:\n",
    "        if col not in ['image', 'label'] and multimodal_df[col].dtype == 'object':\n",
    "            sample_text = str(multimodal_df[col].iloc[0])\n",
    "            if len(sample_text.split()) > 2:\n",
    "                text_columns.append(col)\n",
    "    \n",
    "    print(f\"Identified text columns: {text_columns}\")\n",
    "    \n",
    "    # Combine text columns into a single description\n",
    "    if text_columns:\n",
    "        multimodal_df['text_description'] = multimodal_df[text_columns].apply(\n",
    "            lambda row: ' '.join([str(val) for val in row.values if pd.notna(val)]),\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        multimodal_df['text_description'] = \"Fashion product\"\n",
    "    \n",
    "    # Keep only essential columns\n",
    "    essential_columns = ['image', 'text_description', 'label']\n",
    "    multimodal_df = multimodal_df[essential_columns].copy()\n",
    "    multimodal_df['text_description'] = multimodal_df['text_description'].fillna('Fashion product')\n",
    "    \n",
    "    print(f\"\\nPrepared multimodal dataset with {len(multimodal_df)} products\")\n",
    "    print(\"\\nSample text descriptions:\")\n",
    "    for i in range(min(3, len(multimodal_df))):\n",
    "        text = multimodal_df['text_description'].iloc[i]\n",
    "        print(f\"   {i+1}. {text[:80]}...\" if len(text) > 80 else f\"   {i+1}. {text}\")\n",
    "    \n",
    "    return multimodal_df\n",
    "\n",
    "# Prepare multimodal data\n",
    "if prepared_fashion_df is not None:\n",
    "    multimodal_fashion_df = prepare_multimodal_data(prepared_fashion_df)\n",
    "else:\n",
    "    print(\"Dataset not ready for multimodal preparation\")\n",
    "    multimodal_fashion_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multimodal classifier (images + text) \n",
    "def train_multimodal_classifier(df, preset='high_quality', time_limit=2400):\n",
    "    \"\"\"Train a multimodal fashion classifier using images and text\"\"\"\n",
    "    \n",
    "    if df is None or len(df) == 0:\n",
    "        print(\"No multimodal data available for training\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Training multimodal fashion classifier (Images + Text)\")\n",
    "    print(f\"Dataset size: {len(df)} products\")\n",
    "    print(f\"Preset: {preset}\")\n",
    "    print(f\"Time limit: {time_limit // 60} minutes\")\n",
    "    \n",
    "    # Filter classes with sufficient samples\n",
    "    class_counts = df['label'].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 2].index\n",
    "    df = df[df['label'].isin(valid_classes)].copy()\n",
    "    \n",
    "    # Split data\n",
    "    try:\n",
    "        train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    except ValueError:\n",
    "        train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"\\n   Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "    \n",
    "    # Initialize multimodal predictor\n",
    "    print(\"\\nInitializing MultiModalPredictor for images + text...\")\n",
    "    \n",
    "    multimodal_predictor = MultiModalPredictor(\n",
    "        label='label',\n",
    "        path='./multimodal_fashion_classifier',\n",
    "        eval_metric='accuracy',\n",
    "        problem_type='classification'\n",
    "    )\n",
    "    \n",
    "    # Train with both images and text\n",
    "    print(\"\\nTraining multimodal model...\")\n",
    "    print(\"AutoGluon will automatically:\")\n",
    "    print(\"   - Process images with computer vision models\")\n",
    "    print(\"   - Process text with language models\")\n",
    "    print(\"   - Apply cross-modal alignment\")\n",
    "    print(\"   - Combine features for improved predictions\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        multimodal_predictor.fit(\n",
    "            train_df,\n",
    "            tuning_data=val_df if len(val_df) >= 3 else None,\n",
    "            time_limit=time_limit,\n",
    "            presets=preset\n",
    "        )\n",
    "        \n",
    "        training_time = datetime.now() - start_time\n",
    "        print(f\"\\nMultimodal training completed in {training_time}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        multimodal_results = multimodal_predictor.evaluate(test_df)\n",
    "        \n",
    "        print(\"\\nMultimodal Test Results:\")\n",
    "        for metric, value in multimodal_results.items():\n",
    "            print(f\"   {metric}: {value:.4f}\")\n",
    "        \n",
    "        return multimodal_predictor, test_df, multimodal_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Multimodal training failed: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Train multimodal classifier\n",
    "if multimodal_fashion_df is not None:\n",
    "    sample_size = min(500, len(multimodal_fashion_df))\n",
    "    demo_multimodal_df = multimodal_fashion_df.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    print(f\"Training multimodal model on {sample_size} products\\n\")\n",
    "    \n",
    "    multimodal_predictor, multimodal_test_data, multimodal_results = train_multimodal_classifier(\n",
    "        demo_multimodal_df,\n",
    "        preset='high_quality',\n",
    "        time_limit=2400\n",
    "    )\n",
    "else:\n",
    "    print(\"Multimodal dataset not ready\")\n",
    "    multimodal_predictor, multimodal_test_data, multimodal_results = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "id": "tq19xcmsrni",
   "source": "# Figure 10-4: Multimodal Data Fusion Architecture\ndef create_multimodal_diagram():\n    \"\"\"Create Figure 10-4: Multimodal Learning Architecture\"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n    ax.set_xlim(0, 14)\n    ax.set_ylim(0, 6)\n    ax.axis('off')\n    \n    # Input modalities (left side)\n    modalities = [\n        {'y': 4.5, 'label': 'Product\\nImage', 'color': '#BBDEFB', 'icon': 'CNN/ViT'},\n        {'y': 3.0, 'label': 'Title/\\nDescription', 'color': '#C8E6C9', 'icon': 'Transformer'},\n        {'y': 1.5, 'label': 'Metadata\\n(Price, Brand)', 'color': '#FFE0B2', 'icon': 'Tabular'},\n    ]\n    \n    for mod in modalities:\n        rect = FancyBboxPatch((0.5, mod['y'] - 0.6), 2.5, 1.2, \n                              boxstyle=\"round,pad=0.05,rounding_size=0.2\",\n                              facecolor=mod['color'], edgecolor='#333333', linewidth=2)\n        ax.add_patch(rect)\n        ax.text(1.75, mod['y'] + 0.1, mod['label'], ha='center', va='center', \n                fontsize=10, fontweight='bold', color='#333333')\n        ax.text(1.75, mod['y'] - 0.35, f\"({mod['icon']})\", ha='center', va='center', \n                fontsize=8, color='#666666')\n    \n    # Fusion arrows\n    for mod in modalities:\n        ax.annotate('', xy=(5.2, 3), xytext=(3.1, mod['y']),\n                   arrowprops=dict(arrowstyle='->', color='#666666', lw=1.5,\n                                 connectionstyle='arc3,rad=0'))\n    \n    # Combined Model (center)\n    rect = FancyBboxPatch((5.2, 2), 3.5, 2, \n                          boxstyle=\"round,pad=0.05,rounding_size=0.3\",\n                          facecolor='#E1BEE7', edgecolor='#7B1FA2', linewidth=3)\n    ax.add_patch(rect)\n    ax.text(6.95, 3.2, 'Multimodal\\nFusion Model', ha='center', va='center', \n            fontsize=12, fontweight='bold', color='#4A148C')\n    ax.text(6.95, 2.4, '(Cross-modal Alignment)', ha='center', va='center', \n            fontsize=9, color='#7B1FA2')\n    \n    # Arrow to output\n    ax.annotate('', xy=(11, 3), xytext=(8.8, 3),\n               arrowprops=dict(arrowstyle='->', color='#666666', lw=2))\n    \n    # Output (right side)\n    rect = FancyBboxPatch((11, 2), 2.5, 2, \n                          boxstyle=\"round,pad=0.05,rounding_size=0.2\",\n                          facecolor='#FFCDD2', edgecolor='#C62828', linewidth=2)\n    ax.add_patch(rect)\n    ax.text(12.25, 3, 'Enhanced\\nProduct\\nClassification', ha='center', va='center', \n            fontsize=10, fontweight='bold', color='#B71C1C')\n    \n    ax.text(7, 5.5, 'Figure 10-4: Multimodal Learning - Combining Visual and Textual Features', \n            ha='center', va='center', fontsize=14, fontweight='bold', color='#1a1a1a')\n    \n    # Add cross-modal alignment note\n    ax.text(7, 0.5, 'Cross-modal alignment learns semantic connections: \"red\" in text ↔ red pixels in image', \n            ha='center', va='center', fontsize=9, style='italic', color='#555555')\n    \n    plt.tight_layout()\n    plt.savefig('figure_10_4_multimodal.png', dpi=150, bbox_inches='tight', \n                facecolor='white', edgecolor='none')\n    plt.show()\n    print(\"Figure 10-4 saved as 'figure_10_4_multimodal.png'\")\n\ncreate_multimodal_diagram()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Handling Missing Modalities in Production\n\nBetter handling of missing modalities allows the system to work effectively even when some data types are unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate handling of missing modalities\n",
    "def test_missing_modalities(predictor, sample_data):\n",
    "    \"\"\"Test model behavior with missing data\"\"\"\n",
    "    \n",
    "    if predictor is None or sample_data is None:\n",
    "        print(\"Predictor or sample data not available\")\n",
    "        return\n",
    "    \n",
    "    print(\"Testing Missing Modality Handling\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nIn production, data is often incomplete. AutoGluon handles this gracefully.\")\n",
    "    \n",
    "    # Test with complete data\n",
    "    sample = sample_data.head(1).copy()\n",
    "    \n",
    "    try:\n",
    "        # Complete data prediction\n",
    "        complete_pred = predictor.predict(sample)\n",
    "        print(f\"\\nComplete data prediction: {complete_pred.iloc[0]}\")\n",
    "        \n",
    "        # Simulate missing text\n",
    "        if 'text_description' in sample.columns:\n",
    "            sample_missing_text = sample.copy()\n",
    "            sample_missing_text['text_description'] = \"\"\n",
    "            try:\n",
    "                missing_text_pred = predictor.predict(sample_missing_text)\n",
    "                print(f\"Missing text prediction: {missing_text_pred.iloc[0]}\")\n",
    "            except:\n",
    "                print(\"Missing text: Model handled gracefully (used image only)\")\n",
    "        \n",
    "        print(\"\\nKey Insight: AutoGluon can make predictions even with incomplete data,\")\n",
    "        print(\"which is crucial for real-world deployment where data quality varies.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {e}\")\n",
    "\n",
    "# Test if multimodal predictor is available\n",
    "if multimodal_predictor is not None and multimodal_test_data is not None:\n",
    "    test_missing_modalities(multimodal_predictor, multimodal_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare image-only vs. multimodal performance\n",
    "def compare_model_performance(image_results, multimodal_results):\n",
    "    \"\"\"Compare performance between image-only and multimodal models\"\"\"\n",
    "    \n",
    "    if image_results is None or multimodal_results is None:\n",
    "        print(\"Cannot compare - one or both models not trained\")\n",
    "        return\n",
    "    \n",
    "    print(\"Model Performance Comparison\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    \n",
    "    for metric in image_results.keys():\n",
    "        if metric in multimodal_results:\n",
    "            image_val = image_results[metric]\n",
    "            multimodal_val = multimodal_results[metric]\n",
    "            improvement = ((multimodal_val - image_val) / image_val) * 100 if image_val != 0 else 0\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Metric': metric,\n",
    "                'Image Only': f\"{image_val:.4f}\",\n",
    "                'Multimodal': f\"{multimodal_val:.4f}\",\n",
    "                'Improvement': f\"{improvement:+.2f}%\"\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Visualize comparison\n",
    "    if comparison_data:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        metrics = [row['Metric'] for row in comparison_data]\n",
    "        image_values = [float(row['Image Only']) for row in comparison_data]\n",
    "        multimodal_values = [float(row['Multimodal']) for row in comparison_data]\n",
    "        \n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, image_values, width, label='Image Only', alpha=0.8, color='steelblue')\n",
    "        bars2 = ax.bar(x + width/2, multimodal_values, width, label='Multimodal', alpha=0.8, color='darkorange')\n",
    "        \n",
    "        ax.set_xlabel('Metrics')\n",
    "        ax.set_ylabel('Performance')\n",
    "        ax.set_title('Image-Only vs. Multimodal Performance Comparison')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(metrics)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Key insights\n",
    "    print(\"\\nKey Insights:\")\n",
    "    avg_improvement = np.mean([float(row['Improvement'].strip('%')) for row in comparison_data])\n",
    "    print(f\"   Average improvement: {avg_improvement:+.2f}%\")\n",
    "    \n",
    "    if avg_improvement > 0:\n",
    "        print(\"   Multimodal approach shows improvements\")\n",
    "        print(\"   Combining images with text provides valuable additional context\")\n",
    "\n",
    "# Compare models\n",
    "if results is not None and multimodal_results is not None:\n",
    "    compare_model_performance(results, multimodal_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete E-commerce Project\n\nLet's build a comprehensive computer vision system that demonstrates the full range of AutoGluon's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "id": "wapba3ntsu",
   "source": "# NOTE: This visualization uses illustrative sample data, not actual model results\n# Figure 10-8: Model Performance Across Product Categories\ndef create_category_performance_chart():\n    \"\"\"\n    Create Figure 10-8: Performance metrics chart showing accuracy, precision, \n    and recall for different product categories\n    \"\"\"\n    \n    # Sample performance data by category\n    categories = ['Electronics', 'Clothing', 'Home & Garden', 'Sports', 'Books', 'Toys']\n    \n    # Simulated metrics (in production, these would come from model evaluation)\n    np.random.seed(42)\n    accuracy = [0.94, 0.91, 0.88, 0.92, 0.95, 0.89]\n    precision = [0.93, 0.89, 0.86, 0.91, 0.94, 0.87]\n    recall = [0.95, 0.92, 0.90, 0.93, 0.96, 0.91]\n    \n    # Create figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Left plot: Grouped bar chart for metrics by category\n    ax1 = axes[0]\n    x = np.arange(len(categories))\n    width = 0.25\n    \n    bars1 = ax1.bar(x - width, accuracy, width, label='Accuracy', color='#2196F3', alpha=0.8)\n    bars2 = ax1.bar(x, precision, width, label='Precision', color='#4CAF50', alpha=0.8)\n    bars3 = ax1.bar(x + width, recall, width, label='Recall', color='#FF9800', alpha=0.8)\n    \n    ax1.set_xlabel('Product Category', fontsize=12, fontweight='bold')\n    ax1.set_ylabel('Score', fontsize=12, fontweight='bold')\n    ax1.set_title('Performance Metrics by Category', fontsize=14, fontweight='bold')\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(categories, rotation=45, ha='right')\n    ax1.legend(loc='lower right')\n    ax1.set_ylim(0.7, 1.0)\n    ax1.grid(True, alpha=0.3, axis='y')\n    \n    # Add value labels on bars\n    for bars in [bars1, bars2, bars3]:\n        for bar in bars:\n            height = bar.get_height()\n            ax1.annotate(f'{height:.2f}',\n                        xy=(bar.get_x() + bar.get_width() / 2, height),\n                        xytext=(0, 3), textcoords=\"offset points\",\n                        ha='center', va='bottom', fontsize=8)\n    \n    # Right plot: Confidence level distribution\n    ax2 = axes[1]\n    \n    # Simulated confidence distribution data\n    confidence_levels = ['High\\n(>90%)', 'Medium\\n(70-90%)', 'Low\\n(<70%)']\n    category_conf = {\n        'Electronics': [85, 12, 3],\n        'Clothing': [78, 18, 4],\n        'Home & Garden': [72, 22, 6],\n        'Sports': [80, 16, 4],\n        'Books': [88, 10, 2],\n        'Toys': [75, 20, 5],\n    }\n    \n    x = np.arange(len(confidence_levels))\n    width = 0.12\n    colors = ['#1976D2', '#7B1FA2', '#388E3C', '#F57C00', '#5D4037', '#D32F2F']\n    \n    for i, (cat, values) in enumerate(category_conf.items()):\n        ax2.bar(x + i * width, values, width, label=cat, color=colors[i], alpha=0.8)\n    \n    ax2.set_xlabel('Confidence Level', fontsize=12, fontweight='bold')\n    ax2.set_ylabel('Percentage of Predictions (%)', fontsize=12, fontweight='bold')\n    ax2.set_title('Prediction Confidence Distribution by Category', fontsize=14, fontweight='bold')\n    ax2.set_xticks(x + width * 2.5)\n    ax2.set_xticklabels(confidence_levels)\n    ax2.legend(loc='upper right', fontsize=9, ncol=2)\n    ax2.grid(True, alpha=0.3, axis='y')\n    \n    fig.suptitle('Figure 10-8: Model Performance Across Product Categories', \n                 fontsize=16, fontweight='bold', y=1.02)\n    \n    plt.tight_layout()\n    plt.savefig('figure_10_8_category_performance.png', dpi=150, bbox_inches='tight', \n                facecolor='white', edgecolor='none')\n    plt.show()\n    print(\"Figure 10-8 saved as 'figure_10_8_category_performance.png'\")\n\n# Generate Figure 10-8\ncreate_category_performance_chart()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Production Deployment and Best Practices\n\nFor production deployment, consider performance optimizations, monitoring, and maintenance procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource-constrained training configuration\n",
    "print(\"Resource-Constrained Training Configuration\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Example hyperparameters for limited GPU memory\n",
    "resource_constrained_config = {\n",
    "    'optimization.learning_rate': 1e-4,\n",
    "    'optimization.max_epochs': 20,\n",
    "    'env.per_gpu_batch_size': 8,        # What your GPU can handle\n",
    "    'env.batch_size': 32,                # Effective batch size you want\n",
    "    'env.precision': 'bf16-mixed',       # Mixed precision for efficiency\n",
    "    'optimization.gradient_accumulation_steps': 4  # 8 * 4 = 32 effective batch size\n",
    "}\n",
    "\n",
    "print(\"\\nFor limited GPU memory, use gradient accumulation:\")\n",
    "print(\"\")\n",
    "print(\"   hyperparameters={\")\n",
    "for key, value in resource_constrained_config.items():\n",
    "    print(f\"       '{key}': {value},\")\n",
    "print(\"   }\")\n",
    "\n",
    "print(\"\\n\\nHow Gradient Accumulation Works:\")\n",
    "print(\"   - GPU can only handle batch_size=8 at a time\")\n",
    "print(\"   - With gradient_accumulation_steps=4:\")\n",
    "print(\"     - Run 4 forward passes with batch_size=8 each\")\n",
    "print(\"     - Accumulate gradients from all 4 passes\")\n",
    "print(\"     - Update weights once (effective batch_size = 8 * 4 = 32)\")\n",
    "print(\"\\n   This achieves similar training dynamics to batch_size=32\")\n",
    "print(\"   without requiring 4x more GPU memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production model deployment\n",
    "def demonstrate_production_deployment(predictor, model_name=\"fashion_classifier\"):\n",
    "    \"\"\"Demonstrate production deployment workflow\"\"\"\n",
    "    \n",
    "    if predictor is None:\n",
    "        print(\"No predictor available for deployment demonstration\")\n",
    "        return\n",
    "    \n",
    "    print(\"Production Deployment Workflow\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Save model\n",
    "    save_path = f'./production_{model_name}'\n",
    "    predictor.save(save_path)\n",
    "    print(f\"\\n1. Model saved to: {save_path}\")\n",
    "    \n",
    "    # Demonstrate loading\n",
    "    print(\"\\n2. Loading model for inference:\")\n",
    "    print(f\"   production_predictor = MultiModalPredictor.load('{save_path}')\")\n",
    "    \n",
    "    # Batch prediction pattern\n",
    "    print(\"\\n3. Batch prediction for production efficiency:\")\n",
    "    print(\"   batch_predictions = predictor.predict(new_products_batch)\")\n",
    "    print(\"   confidence_scores = predictor.predict_proba(new_products_batch)\")\n",
    "    \n",
    "    # Confidence-based routing\n",
    "    print(\"\\n4. Confidence-based routing:\")\n",
    "    print(\"   # Filter predictions by confidence threshold\")\n",
    "    print(\"   high_confidence_mask = confidence_scores.max(axis=1) > 0.85\")\n",
    "    print(\"   auto_categorized = predictions[high_confidence_mask]\")\n",
    "    print(\"   needs_human_review = predictions[~high_confidence_mask]\")\n",
    "    \n",
    "    print(\"\\n5. Performance metrics to monitor:\")\n",
    "    print(\"   - Prediction latency (aim for <100ms per image)\")\n",
    "    print(\"   - Confidence distribution over time\")\n",
    "    print(\"   - Category distribution shifts (data drift)\")\n",
    "    print(\"   - Human override rate for low-confidence predictions\")\n",
    "\n",
    "# Demonstrate deployment\n",
    "if predictor is not None:\n",
    "    demonstrate_production_deployment(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "id": "p5lst51r9xp",
   "source": "# NOTE: This visualization uses illustrative sample data, not actual model results\n# Figure 10-9: Model Monitoring Dashboard for E-commerce CV Systems\ndef create_monitoring_dashboard():\n    \"\"\"\n    Create Figure 10-9: A comprehensive monitoring dashboard showing:\n    - Model accuracy over time\n    - Confidence distribution\n    - Category performance\n    - Alert indicators\n    \"\"\"\n    \n    fig = plt.figure(figsize=(18, 12))\n    \n    # Create grid layout\n    gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n    \n    # ============================================\n    # Panel 1: Model Accuracy Over Time (top-left, spans 2 columns)\n    # ============================================\n    ax1 = fig.add_subplot(gs[0, :2])\n    \n    # Generate time series data\n    np.random.seed(42)\n    days = pd.date_range(start='2024-01-01', periods=90, freq='D')\n    base_accuracy = 0.92\n    accuracy_trend = base_accuracy + np.cumsum(np.random.randn(90) * 0.002)\n    accuracy_trend = np.clip(accuracy_trend, 0.85, 0.98)\n    \n    ax1.plot(days, accuracy_trend, 'b-', linewidth=2, label='Model Accuracy')\n    ax1.axhline(y=0.90, color='orange', linestyle='--', linewidth=1.5, label='Warning Threshold')\n    ax1.axhline(y=0.85, color='red', linestyle='--', linewidth=1.5, label='Critical Threshold')\n    ax1.fill_between(days, accuracy_trend, 0.85, where=(accuracy_trend < 0.90), \n                     color='orange', alpha=0.3)\n    ax1.fill_between(days, accuracy_trend, 0.85, where=(accuracy_trend < 0.85), \n                     color='red', alpha=0.3)\n    \n    ax1.set_xlabel('Date', fontsize=11)\n    ax1.set_ylabel('Accuracy', fontsize=11)\n    ax1.set_title('Model Accuracy Over Time', fontsize=13, fontweight='bold')\n    ax1.legend(loc='lower left', fontsize=9)\n    ax1.set_ylim(0.80, 1.0)\n    ax1.grid(True, alpha=0.3)\n    ax1.tick_params(axis='x', rotation=45)\n    \n    # ============================================\n    # Panel 2: Alert Status (top-right)\n    # ============================================\n    ax2 = fig.add_subplot(gs[0, 2])\n    ax2.axis('off')\n    \n    # Create alert status panel\n    alerts = [\n        {'name': 'Model Accuracy', 'status': 'OK', 'color': '#4CAF50', 'value': '92.3%'},\n        {'name': 'Avg Confidence', 'status': 'OK', 'color': '#4CAF50', 'value': '87.5%'},\n        {'name': 'Drift Detection', 'status': 'WARNING', 'color': '#FF9800', 'value': '+2.1%'},\n        {'name': 'Latency P99', 'status': 'OK', 'color': '#4CAF50', 'value': '45ms'},\n        {'name': 'Error Rate', 'status': 'OK', 'color': '#4CAF50', 'value': '0.02%'},\n    ]\n    \n    ax2.text(0.5, 0.95, 'System Status', ha='center', va='top', \n             fontsize=14, fontweight='bold', transform=ax2.transAxes)\n    \n    for i, alert in enumerate(alerts):\n        y_pos = 0.82 - i * 0.16\n        # Status indicator circle\n        circle = plt.Circle((0.08, y_pos), 0.04, color=alert['color'], transform=ax2.transAxes)\n        ax2.add_patch(circle)\n        # Alert name and value\n        ax2.text(0.15, y_pos, alert['name'], ha='left', va='center', \n                fontsize=10, transform=ax2.transAxes)\n        ax2.text(0.85, y_pos, alert['value'], ha='right', va='center', \n                fontsize=10, fontweight='bold', transform=ax2.transAxes)\n        ax2.text(0.95, y_pos, alert['status'], ha='right', va='center', \n                fontsize=9, color=alert['color'], fontweight='bold', transform=ax2.transAxes)\n    \n    # ============================================\n    # Panel 3: Confidence Distribution (middle-left)\n    # ============================================\n    ax3 = fig.add_subplot(gs[1, 0])\n    \n    # Generate confidence score distribution\n    confidence_scores = np.concatenate([\n        np.random.beta(8, 2, 700),   # High confidence (majority)\n        np.random.beta(5, 5, 200),   # Medium confidence\n        np.random.beta(2, 5, 100),   # Low confidence\n    ])\n    \n    ax3.hist(confidence_scores, bins=30, color='#2196F3', alpha=0.7, edgecolor='white')\n    ax3.axvline(x=0.9, color='green', linestyle='--', linewidth=2, label='Auto-approve (>90%)')\n    ax3.axvline(x=0.7, color='orange', linestyle='--', linewidth=2, label='Review (<70%)')\n    ax3.set_xlabel('Confidence Score', fontsize=11)\n    ax3.set_ylabel('Count', fontsize=11)\n    ax3.set_title('Prediction Confidence Distribution', fontsize=13, fontweight='bold')\n    ax3.legend(loc='upper left', fontsize=9)\n    ax3.grid(True, alpha=0.3, axis='y')\n    \n    # ============================================\n    # Panel 4: Predictions by Category (middle-center)\n    # ============================================\n    ax4 = fig.add_subplot(gs[1, 1])\n    \n    categories = ['Electronics', 'Clothing', 'Home', 'Sports', 'Books', 'Toys']\n    volumes = [2450, 1890, 1230, 980, 750, 620]\n    colors = ['#1976D2', '#7B1FA2', '#388E3C', '#F57C00', '#5D4037', '#D32F2F']\n    \n    bars = ax4.barh(categories, volumes, color=colors, alpha=0.8)\n    ax4.set_xlabel('Predictions (Last 24h)', fontsize=11)\n    ax4.set_title('Predictions by Category', fontsize=13, fontweight='bold')\n    ax4.grid(True, alpha=0.3, axis='x')\n    \n    # Add value labels\n    for bar, vol in zip(bars, volumes):\n        ax4.text(vol + 50, bar.get_y() + bar.get_height()/2, \n                f'{vol:,}', va='center', fontsize=9)\n    \n    # ============================================\n    # Panel 5: Hourly Throughput (middle-right)\n    # ============================================\n    ax5 = fig.add_subplot(gs[1, 2])\n    \n    hours = list(range(24))\n    throughput = [120, 85, 45, 30, 25, 35, 80, 250, 480, 520, 490, 450,\n                  380, 420, 510, 550, 520, 480, 350, 280, 220, 180, 160, 140]\n    \n    ax5.fill_between(hours, throughput, alpha=0.4, color='#2196F3')\n    ax5.plot(hours, throughput, 'b-', linewidth=2)\n    ax5.set_xlabel('Hour of Day', fontsize=11)\n    ax5.set_ylabel('Predictions/min', fontsize=11)\n    ax5.set_title('Hourly Throughput', fontsize=13, fontweight='bold')\n    ax5.set_xticks([0, 6, 12, 18, 23])\n    ax5.grid(True, alpha=0.3)\n    \n    # ============================================\n    # Panel 6: Category Accuracy Trends (bottom, spans all columns)\n    # ============================================\n    ax6 = fig.add_subplot(gs[2, :])\n    \n    weeks = list(range(1, 13))\n    cat_accuracy = {\n        'Electronics': [0.94, 0.93, 0.94, 0.95, 0.94, 0.93, 0.94, 0.95, 0.94, 0.93, 0.94, 0.95],\n        'Clothing': [0.91, 0.90, 0.91, 0.92, 0.91, 0.90, 0.89, 0.90, 0.91, 0.92, 0.91, 0.90],\n        'Home & Garden': [0.88, 0.87, 0.88, 0.89, 0.88, 0.87, 0.88, 0.89, 0.90, 0.89, 0.88, 0.89],\n        'Sports': [0.92, 0.91, 0.92, 0.93, 0.92, 0.91, 0.92, 0.93, 0.92, 0.91, 0.92, 0.93],\n    }\n    \n    colors = ['#1976D2', '#7B1FA2', '#388E3C', '#F57C00']\n    for (cat, acc), color in zip(cat_accuracy.items(), colors):\n        ax6.plot(weeks, acc, '-o', label=cat, color=color, linewidth=2, markersize=6)\n    \n    ax6.set_xlabel('Week', fontsize=11)\n    ax6.set_ylabel('Accuracy', fontsize=11)\n    ax6.set_title('Category-Specific Accuracy Trends (12 Weeks)', fontsize=13, fontweight='bold')\n    ax6.legend(loc='lower right', fontsize=10, ncol=4)\n    ax6.set_ylim(0.82, 1.0)\n    ax6.grid(True, alpha=0.3)\n    ax6.set_xticks(weeks)\n    \n    fig.suptitle('Figure 10-9: E-commerce Computer Vision Model Monitoring Dashboard', \n                 fontsize=18, fontweight='bold', y=1.01)\n    \n    plt.tight_layout()\n    plt.savefig('figure_10_9_monitoring_dashboard.png', dpi=150, bbox_inches='tight', \n                facecolor='white', edgecolor='none')\n    plt.show()\n    print(\"Figure 10-9 saved as 'figure_10_9_monitoring_dashboard.png'\")\n\n# Generate Figure 10-9\ncreate_monitoring_dashboard()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checklist\n",
    "print(\"Data Quality Checklist for Computer Vision\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "checklist = [\n",
    "    (\"Consistent lighting\", \"All images should have similar lighting conditions, or use augmentation to simulate variations\"),\n",
    "    (\"Consistent resolution\", \"Use same input resolution or resize appropriately\"),\n",
    "    (\"Background consistency\", \"Similar backgrounds, or augment with various backgrounds\"),\n",
    "    (\"Class balance\", \"Check for severely imbalanced classes, use class weights if needed\"),\n",
    "    (\"Training-Production gap\", \"Ensure training data represents production scenarios\"),\n",
    "    (\"Image quality\", \"Remove corrupted or very low quality images\"),\n",
    "    (\"Label accuracy\", \"Verify labels are correct, especially for edge cases\")\n",
    "]\n",
    "\n",
    "for i, (item, description) in enumerate(checklist, 1):\n",
    "    print(f\"\\n{i}. {item}\")\n",
    "    print(f\"   {description}\")\n",
    "\n",
    "print(\"\\n\\nRemember: High-quality, representative training data is the most\")\n",
    "print(\"important factor for computer vision success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Environment Setup**: Configured AutoGluon 1.5.0 for computer vision tasks\n",
    "2. **Image Classification**: Built classifiers with the famous \"three lines of code\"\n",
    "3. **Architecture Understanding**: CNNs vs ViTs and when each excels\n",
    "4. **Multimodal Learning**: Combined images with text for improved accuracy\n",
    "5. **Cross-modal Alignment**: How models learn text-image connections\n",
    "6. **Missing Data Handling**: Production-ready robustness\n",
    "7. **Resource Optimization**: Gradient accumulation for limited GPU memory\n",
    "8. **Data Quality**: Consistency and its impact on performance\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Start with AutoGluon's default configurations\n",
    "- Use stratified splits and handle class imbalance\n",
    "- Consider domain-specific pre-trained models for specialized applications\n",
    "- Implement confidence-based routing in production\n",
    "- Monitor for data drift and performance degradation\n",
    "\n",
    "### Object Detection Metrics Note:\n",
    "\n",
    "When comparing object detection results:\n",
    "- **mAP@0.5**: IoU threshold of 0.5 (more lenient)\n",
    "- **mAP@0.5:0.95**: Averaged across thresholds (stricter, COCO standard)\n",
    "\n",
    "These can differ significantly—always check which metric you're looking at!\n",
    "\n",
    "---\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- [AutoGluon Documentation](https://auto.gluon.ai/)\n",
    "- [Computer Vision Tutorials](https://auto.gluon.ai/stable/tutorials/multimodal/)\n",
    "- [Production Deployment Guide](https://auto.gluon.ai/stable/tutorials/cloud_fit_deploy/)\n",
    "\n",
    "Happy building with AutoGluon!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl-fresh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}