{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Deployment and CI/CD for AutoML Models\n",
    "\n",
    "This notebook accompanies Chapter 13 of the AutoML book, demonstrating practical deployment patterns for AutoML models. We'll cover:\n",
    "\n",
    "1. Model preparation and packaging\n",
    "2. FastAPI serving with health checks\n",
    "3. Shadow deployment implementation\n",
    "4. MLflow model registry integration\n",
    "5. Prometheus metrics collection\n",
    "6. Drift detection with Evidently\n",
    "7. Input sanitization and security\n",
    "8. Continuous learning pipeline\n",
    "\n",
    "**Note**: Many of these examples are designed to run in a containerized environment. The notebook demonstrates the code patterns that you would deploy in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Preparation for Deployment\n",
    "\n",
    "Before deploying, we need to save the model with comprehensive metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "class ModelPackager:\n",
    "    \"\"\"\n",
    "    Packages trained AutoGluon models for deployment.\n",
    "    \n",
    "    Creates:\n",
    "    - Model metadata (features, classes, metrics)\n",
    "    - Requirements file for dependencies\n",
    "    - Validation checks for deployment readiness\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        self.model_path = Path(model_path)\n",
    "        self.metadata = {}\n",
    "    \n",
    "    def create_metadata(self, predictor, test_accuracy: float) -> Dict:\n",
    "        \"\"\"Create comprehensive metadata for deployment.\"\"\"\n",
    "        self.metadata = {\n",
    "            'model_type': 'tabular_classification',\n",
    "            'target': predictor.label,\n",
    "            'classes': list(predictor.class_labels) if hasattr(predictor, 'class_labels') else [],\n",
    "            'features': predictor.feature_metadata_in.get_features(),\n",
    "            'eval_metric': predictor.eval_metric.name if hasattr(predictor.eval_metric, 'name') else str(predictor.eval_metric),\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'training_date': datetime.now().isoformat(),\n",
    "            'autogluon_version': '1.5.0',\n",
    "            'model_path': str(self.model_path),\n",
    "            'feature_count': len(predictor.feature_metadata_in.get_features())\n",
    "        }\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_path = self.model_path / 'metadata.json'\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"Metadata saved to {metadata_path}\")\n",
    "        return self.metadata\n",
    "    \n",
    "    def create_requirements(self) -> str:\n",
    "        \"\"\"Create requirements.txt for the model.\"\"\"\n",
    "        requirements = \"\"\"\n",
    "autogluon.tabular==1.5.0\n",
    "fastapi==0.104.1\n",
    "uvicorn[standard]==0.24.0\n",
    "pydantic==2.5.0\n",
    "pandas==2.1.3\n",
    "numpy==1.26.2\n",
    "python-multipart==0.0.6\n",
    "prometheus-client==0.19.0\n",
    "evidently==0.4.0\n",
    "\"\"\".strip()\n",
    "        \n",
    "        req_path = self.model_path.parent / 'requirements.txt'\n",
    "        with open(req_path, 'w') as f:\n",
    "            f.write(requirements)\n",
    "        \n",
    "        print(f\"Requirements saved to {req_path}\")\n",
    "        return requirements\n",
    "    \n",
    "    def validate_deployment_readiness(self) -> Dict[str, bool]:\n",
    "        \"\"\"Check if model is ready for deployment.\"\"\"\n",
    "        checks = {\n",
    "            'model_exists': self.model_path.exists(),\n",
    "            'metadata_exists': (self.model_path / 'metadata.json').exists(),\n",
    "            'predictor_exists': (self.model_path / 'predictor.pkl').exists() or \n",
    "                               (self.model_path / 'learner.pkl').exists(),\n",
    "        }\n",
    "        \n",
    "        all_passed = all(checks.values())\n",
    "        print(f\"Deployment readiness: {'PASSED' if all_passed else 'FAILED'}\")\n",
    "        for check, passed in checks.items():\n",
    "            print(f\"  {check}: {'\u2713' if passed else '\u2717'}\")\n",
    "        \n",
    "        return checks\n",
    "\n",
    "# Example usage (would run after training)\n",
    "print(\"ModelPackager class defined for deployment preparation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FastAPI Serving with Comprehensive Health Checks\n",
    "\n",
    "Health checks should verify not just that the model is loaded, but that it can actually make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class HealthStatus:\n",
    "    \"\"\"Health check status.\"\"\"\n",
    "    status: str\n",
    "    model_loaded: bool\n",
    "    can_predict: bool\n",
    "    model_info: Optional[Dict] = None\n",
    "    error: Optional[str] = None\n",
    "    latency_ms: Optional[float] = None\n",
    "\n",
    "class ModelServer:\n",
    "    \"\"\"\n",
    "    Production model server with comprehensive health checks.\n",
    "    \n",
    "    Addresses reviewer comment [f]: Health checks should make\n",
    "    test predictions to verify model integrity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standard test input for health checks\n",
    "    TEST_INPUT = {\n",
    "        \"age\": 35,\n",
    "        \"workclass\": \"Private\",\n",
    "        \"education\": \"Bachelors\",\n",
    "        \"education_num\": 13,\n",
    "        \"marital_status\": \"Never-married\",\n",
    "        \"occupation\": \"Tech-support\",\n",
    "        \"relationship\": \"Not-in-family\",\n",
    "        \"race\": \"White\",\n",
    "        \"sex\": \"Male\",\n",
    "        \"capital_gain\": 0,\n",
    "        \"capital_loss\": 0,\n",
    "        \"hours_per_week\": 40,\n",
    "        \"native_country\": \"United-States\"\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        self.model_path = model_path\n",
    "        self.predictor = None\n",
    "        self.metadata = {}\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the AutoGluon predictor.\"\"\"\n",
    "        # In production, this would be:\n",
    "        # from autogluon.tabular import TabularPredictor\n",
    "        # self.predictor = TabularPredictor.load(self.model_path)\n",
    "        print(f\"Model would be loaded from {self.model_path}\")\n",
    "        self.predictor = \"mock_predictor\"  # Simulated for notebook\n",
    "    \n",
    "    def health_check(self) -> HealthStatus:\n",
    "        \"\"\"\n",
    "        Comprehensive health check.\n",
    "        \n",
    "        Verifies:\n",
    "        1. Model is loaded\n",
    "        2. Model can make predictions (catches corrupted weights)\n",
    "        3. Prediction is valid\n",
    "        \"\"\"\n",
    "        status = HealthStatus(\n",
    "            status=\"unhealthy\",\n",
    "            model_loaded=False,\n",
    "            can_predict=False\n",
    "        )\n",
    "        \n",
    "        # Check 1: Model loaded\n",
    "        if self.predictor is None:\n",
    "            status.error = \"Model not loaded\"\n",
    "            return status\n",
    "        \n",
    "        status.model_loaded = True\n",
    "        status.model_info = self.metadata\n",
    "        \n",
    "        # Check 2: Make test prediction\n",
    "        try:\n",
    "            start = time.time()\n",
    "            # In production: self.predictor.predict(pd.DataFrame([self.TEST_INPUT]))\n",
    "            prediction = \">50K\"  # Simulated\n",
    "            status.latency_ms = (time.time() - start) * 1000\n",
    "            \n",
    "            # Check 3: Validate prediction\n",
    "            valid_classes = ['<=50K', '>50K']\n",
    "            if prediction not in valid_classes:\n",
    "                status.error = f\"Invalid prediction: {prediction}\"\n",
    "                return status\n",
    "            \n",
    "            status.can_predict = True\n",
    "            status.status = \"healthy\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            status.error = f\"Prediction failed: {str(e)}\"\n",
    "        \n",
    "        return status\n",
    "    \n",
    "    def readiness_check(self) -> bool:\n",
    "        \"\"\"\n",
    "        Kubernetes readiness probe.\n",
    "        \n",
    "        Separate from liveness - indicates ready for traffic.\n",
    "        \"\"\"\n",
    "        health = self.health_check()\n",
    "        return health.status == \"healthy\"\n",
    "\n",
    "# Demonstrate health check\n",
    "server = ModelServer(\"/app/models/adult_income_model\")\n",
    "server.load_model()\n",
    "health = server.health_check()\n",
    "print(f\"\\nHealth Status: {health.status}\")\n",
    "print(f\"Model Loaded: {health.model_loaded}\")\n",
    "print(f\"Can Predict: {health.can_predict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Shadow Deployment Implementation\n",
    "\n",
    "Shadow deployments run candidate models against production traffic without affecting users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Callable\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "@dataclass\n",
    "class ShadowComparison:\n",
    "    \"\"\"Record of production vs shadow prediction comparison.\"\"\"\n",
    "    request_id: str\n",
    "    timestamp: datetime\n",
    "    production_prediction: Any\n",
    "    shadow_prediction: Any\n",
    "    production_latency_ms: float\n",
    "    shadow_latency_ms: float\n",
    "    predictions_match: bool\n",
    "\n",
    "class ShadowDeploymentManager:\n",
    "    \"\"\"\n",
    "    Manages shadow deployment for safe model validation.\n",
    "    \n",
    "    Addresses reviewer comment [c]: Show how to implement\n",
    "    shadow mode in FastAPI.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate: float = 1.0):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.comparisons: deque = deque(maxlen=10000)\n",
    "        self.production_model = None\n",
    "        self.shadow_model = None\n",
    "        self._comparison_count = 0\n",
    "        self._discrepancy_count = 0\n",
    "    \n",
    "    def predict_with_shadow(\n",
    "        self,\n",
    "        features: Dict,\n",
    "        production_predict: Callable,\n",
    "        shadow_predict: Callable\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Make production prediction with async shadow comparison.\n",
    "        \n",
    "        Production prediction is synchronous (returned to user).\n",
    "        Shadow prediction is logged but doesn't affect response.\n",
    "        \"\"\"\n",
    "        request_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Production prediction (what user sees)\n",
    "        start = time.time()\n",
    "        production_result = production_predict(features)\n",
    "        production_latency = (time.time() - start) * 1000\n",
    "        \n",
    "        # Shadow prediction (sampled, non-blocking in production)\n",
    "        if random.random() < self.sample_rate:\n",
    "            start = time.time()\n",
    "            shadow_result = shadow_predict(features)\n",
    "            shadow_latency = (time.time() - start) * 1000\n",
    "            \n",
    "            # Log comparison\n",
    "            comparison = ShadowComparison(\n",
    "                request_id=request_id,\n",
    "                timestamp=datetime.now(),\n",
    "                production_prediction=production_result,\n",
    "                shadow_prediction=shadow_result,\n",
    "                production_latency_ms=production_latency,\n",
    "                shadow_latency_ms=shadow_latency,\n",
    "                predictions_match=(production_result == shadow_result)\n",
    "            )\n",
    "            self.comparisons.append(comparison)\n",
    "            self._comparison_count += 1\n",
    "            \n",
    "            if not comparison.predictions_match:\n",
    "                self._discrepancy_count += 1\n",
    "        \n",
    "        return {\n",
    "            \"prediction\": production_result,\n",
    "            \"request_id\": request_id,\n",
    "            \"latency_ms\": production_latency\n",
    "        }\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"Get shadow deployment metrics.\"\"\"\n",
    "        agreement_rate = (\n",
    "            (self._comparison_count - self._discrepancy_count) / \n",
    "            self._comparison_count if self._comparison_count > 0 else 0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"total_comparisons\": self._comparison_count,\n",
    "            \"discrepancies\": self._discrepancy_count,\n",
    "            \"agreement_rate\": f\"{agreement_rate:.2%}\",\n",
    "            \"sample_rate\": self.sample_rate\n",
    "        }\n",
    "\n",
    "# Demonstrate shadow deployment\n",
    "shadow_mgr = ShadowDeploymentManager(sample_rate=1.0)\n",
    "\n",
    "# Simulated models\n",
    "def prod_model(features): return \">50K\" if features.get('education_num', 0) > 12 else \"<=50K\"\n",
    "def shadow_model(features): return \">50K\" if features.get('education_num', 0) > 11 else \"<=50K\"\n",
    "\n",
    "# Run some predictions\n",
    "test_cases = [\n",
    "    {\"education_num\": 16, \"age\": 45},  # Both predict >50K\n",
    "    {\"education_num\": 9, \"age\": 25},   # Both predict <=50K\n",
    "    {\"education_num\": 12, \"age\": 35},  # Discrepancy: boundary case\n",
    "]\n",
    "\n",
    "for features in test_cases:\n",
    "    result = shadow_mgr.predict_with_shadow(features, prod_model, shadow_model)\n",
    "    print(f\"Features: {features} -> {result['prediction']}\")\n",
    "\n",
    "print(f\"\\nShadow Metrics: {shadow_mgr.get_metrics()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MLflow Model Registry Integration\n",
    "\n",
    "Load models from MLflow registry for dynamic updates without redeployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class MLflowModelManager:\n",
    "    \"\"\"\n",
    "    Manages model loading from MLflow Model Registry.\n",
    "    \n",
    "    Addresses reviewer comment [j]: Show loading from\n",
    "    MLflow model registry to tie chapters together.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tracking_uri: str, model_name: str):\n",
    "        self.tracking_uri = tracking_uri\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.model_version = None\n",
    "        self.model_stage = None\n",
    "    \n",
    "    def load_production_model(self):\n",
    "        \"\"\"\n",
    "        Load the current Production-stage model.\n",
    "        \n",
    "        In production:\n",
    "        ```python\n",
    "        import mlflow\n",
    "        mlflow.set_tracking_uri(self.tracking_uri)\n",
    "        \n",
    "        model_uri = f\"models:/{self.model_name}/Production\"\n",
    "        self.model = mlflow.pyfunc.load_model(model_uri)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        self.model_stage = \"Production\"\n",
    "        self.model_version = \"3\"  # Simulated\n",
    "        print(f\"Loaded {self.model_name} v{self.model_version} ({self.model_stage})\")\n",
    "    \n",
    "    def load_staging_model(self) -> bool:\n",
    "        \"\"\"Load staging model for shadow comparison.\"\"\"\n",
    "        try:\n",
    "            # model_uri = f\"models:/{self.model_name}/Staging\"\n",
    "            # self.shadow_model = mlflow.pyfunc.load_model(model_uri)\n",
    "            print(f\"Loaded staging model for shadow comparison\")\n",
    "            return True\n",
    "        except Exception:\n",
    "            print(\"No staging model available\")\n",
    "            return False\n",
    "    \n",
    "    def check_for_new_version(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a new production model is available.\n",
    "        \n",
    "        Enables hot-reloading without container restart.\n",
    "        \"\"\"\n",
    "        # In production: query MLflow client for latest version\n",
    "        # client = MlflowClient()\n",
    "        # versions = client.get_latest_versions(self.model_name, stages=[\"Production\"])\n",
    "        current_version = int(self.model_version) if self.model_version else 0\n",
    "        latest_version = 4  # Simulated\n",
    "        \n",
    "        if latest_version > current_version:\n",
    "            print(f\"New version available: v{latest_version} (current: v{current_version})\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        \"\"\"Get information about the loaded model.\"\"\"\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"version\": self.model_version,\n",
    "            \"stage\": self.model_stage,\n",
    "            \"tracking_uri\": self.tracking_uri\n",
    "        }\n",
    "\n",
    "# Demonstrate MLflow integration\n",
    "mlflow_mgr = MLflowModelManager(\n",
    "    tracking_uri=\"http://mlflow-server:5000\",\n",
    "    model_name=\"adult-income-predictor\"\n",
    ")\n",
    "\n",
    "mlflow_mgr.load_production_model()\n",
    "print(f\"Model Info: {mlflow_mgr.get_model_info()}\")\n",
    "mlflow_mgr.check_for_new_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prometheus Metrics Collection\n",
    "\n",
    "Collect ML-specific metrics for production monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class PrometheusMetricsCollector:\n",
    "    \"\"\"\n",
    "    Collects Prometheus-compatible metrics for ML serving.\n",
    "    \n",
    "    Addresses reviewer comment [k]: Introduce Prometheus-Grafana\n",
    "    stack for production monitoring.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, model_version: str):\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        \n",
    "        # Counters\n",
    "        self.prediction_counts = defaultdict(int)\n",
    "        self.error_counts = defaultdict(int)\n",
    "        \n",
    "        # Histograms (simplified as lists)\n",
    "        self.latencies = []\n",
    "        self.confidences = defaultdict(list)\n",
    "        self.feature_values = defaultdict(list)\n",
    "    \n",
    "    def record_prediction(\n",
    "        self,\n",
    "        prediction: str,\n",
    "        latency_seconds: float,\n",
    "        confidence: float,\n",
    "        features: Dict\n",
    "    ):\n",
    "        \"\"\"Record metrics for a prediction.\"\"\"\n",
    "        # Count predictions by class\n",
    "        self.prediction_counts[prediction] += 1\n",
    "        \n",
    "        # Record latency\n",
    "        self.latencies.append(latency_seconds)\n",
    "        \n",
    "        # Record confidence\n",
    "        self.confidences[prediction].append(confidence)\n",
    "        \n",
    "        # Record feature distributions (numerical only)\n",
    "        for feature, value in features.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                self.feature_values[feature].append(value)\n",
    "    \n",
    "    def record_error(self, error_type: str):\n",
    "        \"\"\"Record a prediction error.\"\"\"\n",
    "        self.error_counts[error_type] += 1\n",
    "    \n",
    "    def get_prometheus_metrics(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate Prometheus exposition format.\n",
    "        \n",
    "        In production, use prometheus_client library.\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        # Prediction counts\n",
    "        lines.append(\"# HELP ml_predictions_total Total predictions\")\n",
    "        lines.append(\"# TYPE ml_predictions_total counter\")\n",
    "        for cls, count in self.prediction_counts.items():\n",
    "            lines.append(\n",
    "                f'ml_predictions_total{{model=\"{self.model_name}\",'\n",
    "                f'version=\"{self.model_version}\",class=\"{cls}\"}} {count}'\n",
    "            )\n",
    "        \n",
    "        # Latency summary\n",
    "        if self.latencies:\n",
    "            import statistics\n",
    "            p50 = statistics.median(self.latencies)\n",
    "            p99 = sorted(self.latencies)[int(len(self.latencies) * 0.99)] if len(self.latencies) > 100 else max(self.latencies)\n",
    "            \n",
    "            lines.append(\"# HELP ml_prediction_latency_seconds Prediction latency\")\n",
    "            lines.append(\"# TYPE ml_prediction_latency_seconds summary\")\n",
    "            lines.append(f'ml_prediction_latency_seconds{{quantile=\"0.5\"}} {p50:.4f}')\n",
    "            lines.append(f'ml_prediction_latency_seconds{{quantile=\"0.99\"}} {p99:.4f}')\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Get metrics summary.\"\"\"\n",
    "        import statistics\n",
    "        \n",
    "        total_predictions = sum(self.prediction_counts.values())\n",
    "        \n",
    "        return {\n",
    "            \"total_predictions\": total_predictions,\n",
    "            \"predictions_by_class\": dict(self.prediction_counts),\n",
    "            \"avg_latency_ms\": statistics.mean(self.latencies) * 1000 if self.latencies else 0,\n",
    "            \"p99_latency_ms\": (sorted(self.latencies)[int(len(self.latencies) * 0.99)] * 1000 \n",
    "                               if len(self.latencies) > 10 else 0),\n",
    "            \"error_counts\": dict(self.error_counts)\n",
    "        }\n",
    "\n",
    "# Demonstrate metrics collection\n",
    "metrics = PrometheusMetricsCollector(\"adult-income\", \"1.2.0\")\n",
    "\n",
    "# Simulate predictions\n",
    "import random\n",
    "for _ in range(100):\n",
    "    prediction = random.choice([\">50K\", \"<=50K\"])\n",
    "    latency = random.uniform(0.01, 0.05)\n",
    "    confidence = random.uniform(0.6, 0.99)\n",
    "    features = {\"age\": random.randint(20, 60), \"education_num\": random.randint(8, 16)}\n",
    "    \n",
    "    metrics.record_prediction(prediction, latency, confidence, features)\n",
    "\n",
    "# Add some errors\n",
    "metrics.record_error(\"validation_error\")\n",
    "metrics.record_error(\"timeout\")\n",
    "\n",
    "print(\"Metrics Summary:\")\n",
    "for key, value in metrics.get_summary().items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nPrometheus Format (excerpt):\")\n",
    "print(metrics.get_prometheus_metrics()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Drift Detection with Evidently\n",
    "\n",
    "Detect data drift that could degrade model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleDriftDetector:\n",
    "    \"\"\"\n",
    "    Simplified drift detection (Evidently-inspired).\n",
    "    \n",
    "    Addresses reviewer comment [l]: Show drift detection\n",
    "    with tools like Evidently, WhyLabs.\n",
    "    \n",
    "    In production, use:\n",
    "    - Evidently: evidently.ai\n",
    "    - WhyLabs: whylabs.ai  \n",
    "    - Great Expectations: greatexpectations.io\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reference_data: pd.DataFrame, feature_columns: List[str]):\n",
    "        self.reference_data = reference_data\n",
    "        self.feature_columns = feature_columns\n",
    "        self.reference_stats = self._compute_stats(reference_data)\n",
    "    \n",
    "    def _compute_stats(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Compute statistics for numerical features.\"\"\"\n",
    "        stats = {}\n",
    "        for col in self.feature_columns:\n",
    "            if df[col].dtype in ['int64', 'float64']:\n",
    "                stats[col] = {\n",
    "                    'mean': df[col].mean(),\n",
    "                    'std': df[col].std(),\n",
    "                    'min': df[col].min(),\n",
    "                    'max': df[col].max()\n",
    "                }\n",
    "        return stats\n",
    "    \n",
    "    def compute_psi(self, expected: np.ndarray, actual: np.ndarray, buckets: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        Compute Population Stability Index (PSI).\n",
    "        \n",
    "        PSI < 0.1: No significant change\n",
    "        0.1 <= PSI < 0.2: Moderate change\n",
    "        PSI >= 0.2: Significant change (drift detected)\n",
    "        \"\"\"\n",
    "        # Create buckets based on expected distribution\n",
    "        breakpoints = np.percentile(expected, np.linspace(0, 100, buckets + 1))\n",
    "        breakpoints[0] = -np.inf\n",
    "        breakpoints[-1] = np.inf\n",
    "        \n",
    "        expected_counts = np.histogram(expected, breakpoints)[0]\n",
    "        actual_counts = np.histogram(actual, breakpoints)[0]\n",
    "        \n",
    "        # Normalize to proportions\n",
    "        expected_props = expected_counts / len(expected)\n",
    "        actual_props = actual_counts / len(actual)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        expected_props = np.where(expected_props == 0, 0.0001, expected_props)\n",
    "        actual_props = np.where(actual_props == 0, 0.0001, actual_props)\n",
    "        \n",
    "        # PSI formula\n",
    "        psi = np.sum((actual_props - expected_props) * np.log(actual_props / expected_props))\n",
    "        return psi\n",
    "    \n",
    "    def check_drift(self, current_data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Check for drift between reference and current data.\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'drift_detected': False,\n",
    "            'drifted_features': [],\n",
    "            'feature_psi': {}\n",
    "        }\n",
    "        \n",
    "        for col in self.feature_columns:\n",
    "            if col not in self.reference_stats:\n",
    "                continue\n",
    "            \n",
    "            ref_values = self.reference_data[col].values\n",
    "            cur_values = current_data[col].values\n",
    "            \n",
    "            psi = self.compute_psi(ref_values, cur_values)\n",
    "            results['feature_psi'][col] = round(psi, 4)\n",
    "            \n",
    "            if psi >= 0.2:\n",
    "                results['drifted_features'].append(col)\n",
    "                results['drift_detected'] = True\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create reference data\n",
    "np.random.seed(42)\n",
    "reference = pd.DataFrame({\n",
    "    'age': np.random.normal(38, 13, 1000).astype(int),\n",
    "    'education_num': np.random.normal(10, 2.5, 1000).astype(int),\n",
    "    'hours_per_week': np.random.normal(40, 12, 1000).astype(int)\n",
    "})\n",
    "\n",
    "# Current data - with drift in 'age'\n",
    "current = pd.DataFrame({\n",
    "    'age': np.random.normal(45, 15, 500).astype(int),  # Shifted!\n",
    "    'education_num': np.random.normal(10, 2.5, 500).astype(int),\n",
    "    'hours_per_week': np.random.normal(40, 12, 500).astype(int)\n",
    "})\n",
    "\n",
    "# Check drift\n",
    "detector = SimpleDriftDetector(reference, ['age', 'education_num', 'hours_per_week'])\n",
    "drift_results = detector.check_drift(current)\n",
    "\n",
    "print(\"Drift Detection Results:\")\n",
    "print(f\"  Drift Detected: {drift_results['drift_detected']}\")\n",
    "print(f\"  Drifted Features: {drift_results['drifted_features']}\")\n",
    "print(f\"  PSI Scores:\")\n",
    "for feature, psi in drift_results['feature_psi'].items():\n",
    "    status = \"DRIFT\" if psi >= 0.2 else \"OK\" if psi < 0.1 else \"WATCH\"\n",
    "    print(f\"    {feature}: {psi:.4f} ({status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Input Sanitization and Security\n",
    "\n",
    "Protect ML endpoints from DoS attacks and malformed inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "@dataclass\n",
    "class SanitizationConfig:\n",
    "    \"\"\"Configuration for input sanitization.\"\"\"\n",
    "    max_request_size_bytes: int = 1_000_000  # 1MB\n",
    "    max_batch_size: int = 100\n",
    "    max_string_length: int = 1000\n",
    "    rate_limit_per_minute: int = 1000\n",
    "\n",
    "class InputSanitizer:\n",
    "    \"\"\"\n",
    "    Sanitizes and validates ML model inputs.\n",
    "    \n",
    "    Addresses reviewer comment [m]: Add input sanitization\n",
    "    and size limits for DoS prevention.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Expected schema for Adult Income model\n",
    "    SCHEMA = {\n",
    "        'age': {'type': int, 'min': 17, 'max': 90},\n",
    "        'education_num': {'type': int, 'min': 1, 'max': 16},\n",
    "        'hours_per_week': {'type': int, 'min': 1, 'max': 99},\n",
    "        'capital_gain': {'type': int, 'min': 0, 'max': 100000},\n",
    "        'capital_loss': {'type': int, 'min': 0, 'max': 5000},\n",
    "        'workclass': {'type': str, 'max_length': 50},\n",
    "        'education': {'type': str, 'max_length': 50},\n",
    "        'marital_status': {'type': str, 'max_length': 50},\n",
    "        'occupation': {'type': str, 'max_length': 50},\n",
    "        'relationship': {'type': str, 'max_length': 50},\n",
    "        'race': {'type': str, 'max_length': 50},\n",
    "        'sex': {'type': str, 'max_length': 10},\n",
    "        'native_country': {'type': str, 'max_length': 50}\n",
    "    }\n",
    "    \n",
    "    def __init__(self, config: SanitizationConfig = None):\n",
    "        self.config = config or SanitizationConfig()\n",
    "    \n",
    "    def validate_batch_size(self, batch: List[Dict]) -> Tuple[bool, str]:\n",
    "        \"\"\"Check if batch size is within limits.\"\"\"\n",
    "        if len(batch) > self.config.max_batch_size:\n",
    "            return False, f\"Batch size {len(batch)} exceeds max {self.config.max_batch_size}\"\n",
    "        return True, \"\"\n",
    "    \n",
    "    def sanitize_features(self, features: Dict) -> Tuple[Dict, List[str]]:\n",
    "        \"\"\"\n",
    "        Sanitize input features.\n",
    "        \n",
    "        Returns:\n",
    "            - Sanitized features dict\n",
    "            - List of warnings/modifications made\n",
    "        \"\"\"\n",
    "        sanitized = {}\n",
    "        warnings = []\n",
    "        \n",
    "        for field, spec in self.SCHEMA.items():\n",
    "            if field not in features:\n",
    "                warnings.append(f\"Missing field: {field}\")\n",
    "                continue\n",
    "            \n",
    "            value = features[field]\n",
    "            \n",
    "            # Type validation and coercion\n",
    "            if spec['type'] == int:\n",
    "                try:\n",
    "                    value = int(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    warnings.append(f\"Invalid int for {field}: {value}\")\n",
    "                    continue\n",
    "                \n",
    "                # Range clamping\n",
    "                if 'min' in spec and value < spec['min']:\n",
    "                    warnings.append(f\"{field} clamped from {value} to {spec['min']}\")\n",
    "                    value = spec['min']\n",
    "                if 'max' in spec and value > spec['max']:\n",
    "                    warnings.append(f\"{field} clamped from {value} to {spec['max']}\")\n",
    "                    value = spec['max']\n",
    "            \n",
    "            elif spec['type'] == str:\n",
    "                value = str(value)\n",
    "                max_len = spec.get('max_length', 100)\n",
    "                if len(value) > max_len:\n",
    "                    warnings.append(f\"{field} truncated from {len(value)} to {max_len} chars\")\n",
    "                    value = value[:max_len]\n",
    "            \n",
    "            sanitized[field] = value\n",
    "        \n",
    "        # Remove unexpected fields (security)\n",
    "        unexpected = set(features.keys()) - set(self.SCHEMA.keys())\n",
    "        if unexpected:\n",
    "            warnings.append(f\"Removed unexpected fields: {unexpected}\")\n",
    "        \n",
    "        return sanitized, warnings\n",
    "    \n",
    "    def detect_adversarial(\n",
    "        self,\n",
    "        features: Dict,\n",
    "        training_stats: Dict,\n",
    "        threshold_std: float = 5.0\n",
    "    ) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"\n",
    "        Basic detection of potentially adversarial inputs.\n",
    "        \n",
    "        Checks if values are far outside training distribution.\n",
    "        \"\"\"\n",
    "        suspicious = []\n",
    "        \n",
    "        for field, stats in training_stats.items():\n",
    "            if field not in features:\n",
    "                continue\n",
    "            \n",
    "            value = features[field]\n",
    "            if not isinstance(value, (int, float)):\n",
    "                continue\n",
    "            \n",
    "            if stats['std'] > 0:\n",
    "                z_score = abs(value - stats['mean']) / stats['std']\n",
    "                if z_score > threshold_std:\n",
    "                    suspicious.append(\n",
    "                        f\"{field}: z-score={z_score:.2f} (value={value})\"\n",
    "                    )\n",
    "        \n",
    "        return len(suspicious) > 0, suspicious\n",
    "\n",
    "# Demonstrate sanitization\n",
    "sanitizer = InputSanitizer()\n",
    "\n",
    "# Test cases\n",
    "test_inputs = [\n",
    "    # Normal input\n",
    "    {\"age\": 35, \"education_num\": 13, \"hours_per_week\": 40,\n",
    "     \"workclass\": \"Private\", \"education\": \"Bachelors\"},\n",
    "    \n",
    "    # Out of range values\n",
    "    {\"age\": 150, \"education_num\": -5, \"hours_per_week\": 200,\n",
    "     \"workclass\": \"Private\"},\n",
    "    \n",
    "    # Injection attempt\n",
    "    {\"age\": 35, \"workclass\": \"Private; DROP TABLE users;--\",\n",
    "     \"malicious_field\": \"<script>alert('xss')</script>\"}\n",
    "]\n",
    "\n",
    "for i, input_data in enumerate(test_inputs):\n",
    "    print(f\"\\nTest {i + 1}:\")\n",
    "    print(f\"  Input: {input_data}\")\n",
    "    sanitized, warnings = sanitizer.sanitize_features(input_data)\n",
    "    print(f\"  Sanitized: {sanitized}\")\n",
    "    if warnings:\n",
    "        print(f\"  Warnings: {warnings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Continuous Learning Pipeline\n",
    "\n",
    "Collect feedback and determine when retraining is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from datetime import timedelta\n",
    "\n",
    "@dataclass\n",
    "class LabeledPrediction:\n",
    "    \"\"\"A prediction with ground truth feedback.\"\"\"\n",
    "    prediction_id: str\n",
    "    features: Dict\n",
    "    prediction: str\n",
    "    ground_truth: Optional[str]\n",
    "    timestamp: datetime\n",
    "\n",
    "class ContinuousLearningPipeline:\n",
    "    \"\"\"\n",
    "    Implements continuous learning from production feedback.\n",
    "    \n",
    "    Addresses reviewer comments [p] and [q]:\n",
    "    - [p]: Ongoing performance monitoring plans\n",
    "    - [q]: Code example for continuous learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        accuracy_threshold: float = 0.80,\n",
    "        drift_threshold: float = 0.2,\n",
    "        min_samples_for_retrain: int = 1000\n",
    "    ):\n",
    "        self.accuracy_threshold = accuracy_threshold\n",
    "        self.drift_threshold = drift_threshold\n",
    "        self.min_samples = min_samples_for_retrain\n",
    "        \n",
    "        self.pending_feedback: Dict[str, LabeledPrediction] = {}\n",
    "        self.labeled_data: deque = deque(maxlen=100000)\n",
    "        self.last_retrain = None\n",
    "    \n",
    "    def record_prediction(self, prediction_id: str, features: Dict, prediction: str):\n",
    "        \"\"\"Record a prediction awaiting feedback.\"\"\"\n",
    "        self.pending_feedback[prediction_id] = LabeledPrediction(\n",
    "            prediction_id=prediction_id,\n",
    "            features=features,\n",
    "            prediction=prediction,\n",
    "            ground_truth=None,\n",
    "            timestamp=datetime.now()\n",
    "        )\n",
    "    \n",
    "    def receive_feedback(self, prediction_id: str, ground_truth: str):\n",
    "        \"\"\"Receive ground truth for a prediction.\"\"\"\n",
    "        if prediction_id not in self.pending_feedback:\n",
    "            return False\n",
    "        \n",
    "        labeled = self.pending_feedback.pop(prediction_id)\n",
    "        labeled.ground_truth = ground_truth\n",
    "        self.labeled_data.append(labeled)\n",
    "        return True\n",
    "    \n",
    "    def get_accuracy(self, window_hours: int = 24) -> float:\n",
    "        \"\"\"Calculate accuracy over recent labeled predictions.\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(hours=window_hours)\n",
    "        recent = [lp for lp in self.labeled_data \n",
    "                  if lp.timestamp > cutoff and lp.ground_truth]\n",
    "        \n",
    "        if not recent:\n",
    "            return 0.0\n",
    "        \n",
    "        correct = sum(1 for lp in recent if lp.prediction == lp.ground_truth)\n",
    "        return correct / len(recent)\n",
    "    \n",
    "    def check_retrain_needed(self, drift_score: float) -> Dict:\n",
    "        \"\"\"\n",
    "        Determine if model retraining should be triggered.\n",
    "        \n",
    "        Checks multiple signals:\n",
    "        - Accuracy below threshold\n",
    "        - Data drift above threshold\n",
    "        - Sufficient new samples collected\n",
    "        \"\"\"\n",
    "        current_accuracy = self.get_accuracy()\n",
    "        new_samples = len(self.labeled_data)\n",
    "        \n",
    "        reasons = []\n",
    "        \n",
    "        if current_accuracy < self.accuracy_threshold and new_samples > 100:\n",
    "            reasons.append(\n",
    "                f\"Accuracy {current_accuracy:.2%} below threshold {self.accuracy_threshold:.2%}\"\n",
    "            )\n",
    "        \n",
    "        if drift_score > self.drift_threshold:\n",
    "            reasons.append(\n",
    "                f\"Drift score {drift_score:.3f} above threshold {self.drift_threshold}\"\n",
    "            )\n",
    "        \n",
    "        if new_samples >= self.min_samples:\n",
    "            reasons.append(\n",
    "                f\"{new_samples} new samples available (threshold: {self.min_samples})\"\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'should_retrain': len(reasons) > 0,\n",
    "            'reasons': reasons,\n",
    "            'current_accuracy': current_accuracy,\n",
    "            'drift_score': drift_score,\n",
    "            'new_samples': new_samples\n",
    "        }\n",
    "    \n",
    "    def select_for_active_learning(self, predictions: List[Tuple[str, float]], n: int = 100) -> List[str]:\n",
    "        \"\"\"\n",
    "        Select samples for human labeling using uncertainty sampling.\n",
    "        \n",
    "        Prioritizes predictions where model is least confident.\n",
    "        \"\"\"\n",
    "        # Sort by confidence (ascending - least confident first)\n",
    "        sorted_preds = sorted(predictions, key=lambda x: x[1])\n",
    "        return [pred_id for pred_id, _ in sorted_preds[:n]]\n",
    "\n",
    "# Demonstrate continuous learning\n",
    "pipeline = ContinuousLearningPipeline(\n",
    "    accuracy_threshold=0.85,\n",
    "    min_samples_for_retrain=50  # Lower for demo\n",
    ")\n",
    "\n",
    "# Simulate predictions and feedback\n",
    "import random\n",
    "for i in range(100):\n",
    "    pred_id = f\"pred_{i}\"\n",
    "    features = {\"age\": random.randint(20, 60)}\n",
    "    prediction = random.choice([\">50K\", \"<=50K\"])\n",
    "    \n",
    "    pipeline.record_prediction(pred_id, features, prediction)\n",
    "    \n",
    "    # Simulate delayed feedback (80% correct)\n",
    "    if random.random() < 0.8:\n",
    "        ground_truth = prediction  # Correct\n",
    "    else:\n",
    "        ground_truth = \">50K\" if prediction == \"<=50K\" else \"<=50K\"  # Wrong\n",
    "    \n",
    "    pipeline.receive_feedback(pred_id, ground_truth)\n",
    "\n",
    "# Check if retraining needed\n",
    "retrain_check = pipeline.check_retrain_needed(drift_score=0.15)\n",
    "\n",
    "print(\"Continuous Learning Pipeline Status:\")\n",
    "print(f\"  Current Accuracy: {pipeline.get_accuracy():.2%}\")\n",
    "print(f\"  Labeled Samples: {len(pipeline.labeled_data)}\")\n",
    "print(f\"  Should Retrain: {retrain_check['should_retrain']}\")\n",
    "if retrain_check['reasons']:\n",
    "    print(f\"  Reasons:\")\n",
    "    for reason in retrain_check['reasons']:\n",
    "        print(f\"    - {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated key deployment patterns for AutoML models:\n",
    "\n",
    "1. **Model Packaging**: Creating metadata and requirements for deployment\n",
    "2. **Health Checks**: Verifying model can actually predict, not just load\n",
    "3. **Shadow Deployment**: Safe validation of new models without user impact\n",
    "4. **MLflow Integration**: Dynamic model loading from registry\n",
    "5. **Prometheus Metrics**: ML-specific monitoring for production\n",
    "6. **Drift Detection**: Using PSI to detect distribution changes\n",
    "7. **Input Sanitization**: Protecting endpoints from attacks\n",
    "8. **Continuous Learning**: Feedback collection and retraining triggers\n",
    "\n",
    "### Production Tools Mentioned\n",
    "\n",
    "- **Serving**: FastAPI, TensorFlow Serving, TorchServe, Triton, BentoML\n",
    "- **Orchestration**: Kubernetes, Istio, Helm\n",
    "- **Monitoring**: Prometheus, Grafana, Datadog\n",
    "- **Drift Detection**: Evidently, WhyLabs, Great Expectations\n",
    "- **Model Registry**: MLflow, Weights & Biases\n",
    "\n",
    "These patterns scale from simple deployments to enterprise ML platforms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}