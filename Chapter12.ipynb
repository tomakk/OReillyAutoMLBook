{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Automating Data Pipelines with Apache Airflow\n",
    "\n",
    "This notebook accompanies Chapter 12 of the O'Reilly AutoML book. It demonstrates practical patterns for building data pipelines that support AutoML workflows, including:\n",
    "\n",
    "1. Data validation and quality gates\n",
    "2. Point-in-time correct feature engineering\n",
    "3. Incremental processing patterns\n",
    "4. Data and concept drift detection\n",
    "5. Data contracts and schema validation\n",
    "\n",
    "**Note:** Some code demonstrates Airflow DAG patterns that would run in an Airflow environment. These are provided for reference and can be adapted to your deployment.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python 3.10+\n",
    "- pandas, numpy, scipy\n",
    "- scikit-learn\n",
    "- (Optional) apache-airflow for full DAG execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas numpy scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Environment ready for pipeline demonstrations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.2: Sample Data Generation\n",
    "\n",
    "We'll create synthetic customer and transaction data to demonstrate pipeline patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample customer data\n",
    "np.random.seed(42)\n",
    "n_customers = 1000\n",
    "\n",
    "customers_df = pd.DataFrame({\n",
    "    'customer_id': [f'CUST_{i:05d}' for i in range(n_customers)],\n",
    "    'registration_date': pd.date_range('2020-01-01', periods=n_customers, freq='2D'),\n",
    "    'customer_segment': np.random.choice(['premium', 'standard', 'basic'], n_customers, p=[0.1, 0.6, 0.3]),\n",
    "    'lifetime_value': np.random.exponential(500, n_customers),\n",
    "    'last_login_date': pd.Timestamp.now() - pd.to_timedelta(np.random.randint(0, 90, n_customers), unit='D'),\n",
    "    'email_verified': np.random.choice([True, False], n_customers, p=[0.9, 0.1]),\n",
    "    'updated_at': pd.Timestamp.now() - pd.to_timedelta(np.random.randint(0, 30, n_customers), unit='D')\n",
    "})\n",
    "\n",
    "print(f\"Created {len(customers_df)} customer records\")\n",
    "customers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample transaction data\n",
    "n_transactions = 10000\n",
    "\n",
    "transactions_df = pd.DataFrame({\n",
    "    'transaction_id': [f'TXN_{i:08d}' for i in range(n_transactions)],\n",
    "    'customer_id': np.random.choice(customers_df['customer_id'], n_transactions),\n",
    "    'transaction_timestamp': pd.date_range('2024-01-01', periods=n_transactions, freq='30min'),\n",
    "    'amount': np.random.lognormal(3, 1, n_transactions),\n",
    "    'product_category': np.random.choice(['electronics', 'clothing', 'food', 'services'], n_transactions)\n",
    "})\n",
    "\n",
    "print(f\"Created {len(transactions_df)} transaction records\")\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.3: Point-in-Time Correct Feature Engineering\n",
    "\n",
    "One of the most critical aspects of ML data pipelines is ensuring **point-in-time correctness** - features should only use data that would have been available at prediction time.\n",
    "\n",
    "### Snippet 12-1: Wrong vs Right Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet 12-1: Point-in-Time Correct Feature Engineering\n",
    "\n",
    "def create_features_WRONG(transactions_df, prediction_date):\n",
    "    \"\"\"\n",
    "    BAD EXAMPLE: Uses ALL transactions, including FUTURE data.\n",
    "    This causes data leakage!\n",
    "    \"\"\"\n",
    "    # BUG: No date filtering - includes future transactions\n",
    "    features = transactions_df.groupby('customer_id').agg({\n",
    "        'amount': ['sum', 'mean', 'count'],\n",
    "        'transaction_timestamp': 'max'\n",
    "    })\n",
    "    features.columns = ['total_spend', 'avg_spend', 'txn_count', 'last_txn']\n",
    "    return features.reset_index()\n",
    "\n",
    "\n",
    "def create_features_CORRECT(transactions_df, prediction_date, lookback_days=30):\n",
    "    \"\"\"\n",
    "    GOOD EXAMPLE: Only uses data BEFORE the prediction date.\n",
    "    This ensures point-in-time correctness.\n",
    "    \"\"\"\n",
    "    # Filter to only historical transactions\n",
    "    cutoff_date = prediction_date\n",
    "    lookback_start = prediction_date - timedelta(days=lookback_days)\n",
    "    \n",
    "    historical_txns = transactions_df[\n",
    "        (transactions_df['transaction_timestamp'] >= lookback_start) &\n",
    "        (transactions_df['transaction_timestamp'] < cutoff_date)  # Strict < not <=\n",
    "    ]\n",
    "    \n",
    "    features = historical_txns.groupby('customer_id').agg({\n",
    "        'amount': ['sum', 'mean', 'count'],\n",
    "        'transaction_timestamp': 'max'\n",
    "    })\n",
    "    features.columns = ['total_spend_30d', 'avg_spend_30d', 'txn_count_30d', 'last_txn_date']\n",
    "    \n",
    "    # Add metadata for auditability\n",
    "    features = features.reset_index()\n",
    "    features['feature_as_of_date'] = prediction_date\n",
    "    features['lookback_days'] = lookback_days\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "# Demonstrate the difference\n",
    "prediction_date = pd.Timestamp('2024-06-15')\n",
    "\n",
    "wrong_features = create_features_WRONG(transactions_df, prediction_date)\n",
    "correct_features = create_features_CORRECT(transactions_df, prediction_date)\n",
    "\n",
    "print(f\"WRONG approach: {len(wrong_features)} customers, uses ALL {len(transactions_df)} transactions\")\n",
    "print(f\"CORRECT approach: {len(correct_features)} customers, uses only historical transactions\")\n",
    "print(f\"\\nCorrect features sample:\")\n",
    "correct_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.4: Data Quality Validation\n",
    "\n",
    "Production pipelines need comprehensive validation to catch data quality issues before they corrupt downstream processing.\n",
    "\n",
    "### Snippet 12-2: Hierarchical Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet 12-2: Hierarchical Data Quality Validation\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"Comprehensive data quality validation with hierarchical checks.\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "        self.results = {\n",
    "            'passed': [],\n",
    "            'failed': [],\n",
    "            'warnings': []\n",
    "        }\n",
    "    \n",
    "    def check_schema(self, required_columns: List[str]) -> bool:\n",
    "        \"\"\"Level 1: Quick schema validation (fail fast).\"\"\"\n",
    "        missing = set(required_columns) - set(self.df.columns)\n",
    "        if missing:\n",
    "            self.results['failed'].append(f\"Missing columns: {missing}\")\n",
    "            return False\n",
    "        self.results['passed'].append(\"Schema validation passed\")\n",
    "        return True\n",
    "    \n",
    "    def check_nulls(self, critical_columns: List[str], max_null_rate: float = 0.05) -> bool:\n",
    "        \"\"\"Level 2: Null rate validation.\"\"\"\n",
    "        all_passed = True\n",
    "        for col in critical_columns:\n",
    "            if col in self.df.columns:\n",
    "                null_rate = self.df[col].isnull().mean()\n",
    "                if null_rate > max_null_rate:\n",
    "                    self.results['failed'].append(\n",
    "                        f\"Column '{col}' null rate {null_rate:.2%} > {max_null_rate:.2%}\"\n",
    "                    )\n",
    "                    all_passed = False\n",
    "        if all_passed:\n",
    "            self.results['passed'].append(\"Null validation passed\")\n",
    "        return all_passed\n",
    "    \n",
    "    def check_duplicates(self, key_columns: List[str]) -> bool:\n",
    "        \"\"\"Level 3: Duplicate detection.\"\"\"\n",
    "        duplicates = self.df.duplicated(subset=key_columns).sum()\n",
    "        if duplicates > 0:\n",
    "            self.results['failed'].append(f\"Found {duplicates} duplicate records\")\n",
    "            return False\n",
    "        self.results['passed'].append(\"Duplicate check passed\")\n",
    "        return True\n",
    "    \n",
    "    def check_value_ranges(self, column: str, min_val: float = None, max_val: float = None) -> bool:\n",
    "        \"\"\"Level 4: Value range validation.\"\"\"\n",
    "        if column not in self.df.columns:\n",
    "            return True\n",
    "        \n",
    "        if min_val is not None:\n",
    "            violations = (self.df[column] < min_val).sum()\n",
    "            if violations > 0:\n",
    "                self.results['warnings'].append(\n",
    "                    f\"{violations} records with {column} < {min_val}\"\n",
    "                )\n",
    "        \n",
    "        if max_val is not None:\n",
    "            violations = (self.df[column] > max_val).sum()\n",
    "            if violations > 0:\n",
    "                self.results['warnings'].append(\n",
    "                    f\"{violations} records with {column} > {max_val}\"\n",
    "                )\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def check_volume(self, min_records: int) -> bool:\n",
    "        \"\"\"Level 5: Volume validation.\"\"\"\n",
    "        if len(self.df) < min_records:\n",
    "            self.results['failed'].append(\n",
    "                f\"Record count {len(self.df)} < minimum {min_records}\"\n",
    "            )\n",
    "            return False\n",
    "        self.results['passed'].append(f\"Volume check passed ({len(self.df)} records)\")\n",
    "        return True\n",
    "    \n",
    "    def validate_all(self, config: Dict[str, Any]) -> Dict:\n",
    "        \"\"\"Run all validations based on config.\"\"\"\n",
    "        # Run checks in order of cost (cheap first)\n",
    "        schema_ok = self.check_schema(config.get('required_columns', []))\n",
    "        if not schema_ok:\n",
    "            return {'success': False, 'results': self.results}\n",
    "        \n",
    "        self.check_volume(config.get('min_records', 1))\n",
    "        self.check_duplicates(config.get('key_columns', []))\n",
    "        self.check_nulls(config.get('critical_columns', []))\n",
    "        \n",
    "        for range_check in config.get('value_ranges', []):\n",
    "            self.check_value_ranges(**range_check)\n",
    "        \n",
    "        return {\n",
    "            'success': len(self.results['failed']) == 0,\n",
    "            'results': self.results\n",
    "        }\n",
    "\n",
    "\n",
    "# Run validation on customer data\n",
    "validation_config = {\n",
    "    'required_columns': ['customer_id', 'customer_segment', 'lifetime_value'],\n",
    "    'key_columns': ['customer_id'],\n",
    "    'critical_columns': ['customer_id', 'customer_segment'],\n",
    "    'min_records': 100,\n",
    "    'value_ranges': [\n",
    "        {'column': 'lifetime_value', 'min_val': 0},\n",
    "    ]\n",
    "}\n",
    "\n",
    "validator = DataValidator(customers_df)\n",
    "validation_result = validator.validate_all(validation_config)\n",
    "\n",
    "print(\"Validation Report:\")\n",
    "print(f\"Success: {validation_result['success']}\")\n",
    "print(f\"\\nPassed: {validation_result['results']['passed']}\")\n",
    "print(f\"Failed: {validation_result['results']['failed']}\")\n",
    "print(f\"Warnings: {validation_result['results']['warnings']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.5: Data Contracts\n",
    "\n",
    "Data contracts provide a systematic approach to handling schema evolution and ensuring data quality at system boundaries.\n",
    "\n",
    "### Snippet 12-3: Data Contract Definition and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet 12-3: Data Contracts\n",
    "\n",
    "@dataclass\n",
    "class FieldDefinition:\n",
    "    \"\"\"Definition of a single field in a data contract.\"\"\"\n",
    "    name: str\n",
    "    dtype: str  # 'string', 'int', 'float', 'datetime', 'bool'\n",
    "    required: bool = True\n",
    "    nullable: bool = False\n",
    "    allowed_values: Optional[List] = None\n",
    "    min_value: Optional[float] = None\n",
    "    max_value: Optional[float] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataContract:\n",
    "    \"\"\"Data contract specification.\"\"\"\n",
    "    name: str\n",
    "    version: str\n",
    "    fields: List[FieldDefinition]\n",
    "    primary_key: List[str]\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Validate a DataFrame against this contract.\"\"\"\n",
    "        violations = []\n",
    "        \n",
    "        # Check required fields\n",
    "        for field in self.fields:\n",
    "            if field.required and field.name not in df.columns:\n",
    "                violations.append(f\"Missing required field: {field.name}\")\n",
    "                continue\n",
    "            \n",
    "            if field.name not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            col = df[field.name]\n",
    "            \n",
    "            # Check nullability\n",
    "            if not field.nullable and col.isnull().any():\n",
    "                null_count = col.isnull().sum()\n",
    "                violations.append(f\"Field '{field.name}' has {null_count} null values\")\n",
    "            \n",
    "            # Check allowed values\n",
    "            if field.allowed_values is not None:\n",
    "                invalid = ~col.dropna().isin(field.allowed_values)\n",
    "                if invalid.any():\n",
    "                    invalid_values = col.dropna()[invalid].unique()[:5]\n",
    "                    violations.append(\n",
    "                        f\"Field '{field.name}' has invalid values: {invalid_values}\"\n",
    "                    )\n",
    "            \n",
    "            # Check numeric ranges\n",
    "            if field.min_value is not None:\n",
    "                below_min = (col < field.min_value).sum()\n",
    "                if below_min > 0:\n",
    "                    violations.append(\n",
    "                        f\"Field '{field.name}' has {below_min} values below {field.min_value}\"\n",
    "                    )\n",
    "        \n",
    "        # Check primary key uniqueness\n",
    "        if all(pk in df.columns for pk in self.primary_key):\n",
    "            duplicates = df.duplicated(subset=self.primary_key).sum()\n",
    "            if duplicates > 0:\n",
    "                violations.append(f\"Primary key has {duplicates} duplicates\")\n",
    "        \n",
    "        return {\n",
    "            'contract': f\"{self.name} v{self.version}\",\n",
    "            'valid': len(violations) == 0,\n",
    "            'violations': violations\n",
    "        }\n",
    "\n",
    "\n",
    "# Define customer data contract\n",
    "CUSTOMER_CONTRACT = DataContract(\n",
    "    name=\"customer_data\",\n",
    "    version=\"1.0\",\n",
    "    fields=[\n",
    "        FieldDefinition('customer_id', 'string', required=True, nullable=False),\n",
    "        FieldDefinition('customer_segment', 'string', required=True, nullable=False,\n",
    "                       allowed_values=['premium', 'standard', 'basic']),\n",
    "        FieldDefinition('lifetime_value', 'float', required=True, nullable=True,\n",
    "                       min_value=0),\n",
    "        FieldDefinition('email_verified', 'bool', required=False, nullable=True),\n",
    "    ],\n",
    "    primary_key=['customer_id']\n",
    ")\n",
    "\n",
    "# Validate our data against the contract\n",
    "contract_result = CUSTOMER_CONTRACT.validate(customers_df)\n",
    "print(f\"Contract: {contract_result['contract']}\")\n",
    "print(f\"Valid: {contract_result['valid']}\")\n",
    "if contract_result['violations']:\n",
    "    print(f\"Violations: {contract_result['violations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.6: Data Drift Detection\n",
    "\n",
    "Monitoring for data drift is essential for production ML systems. We need to detect both **data drift** (input distributions changing) and **concept drift** (the relationship between inputs and outputs changing).\n",
    "\n",
    "### Snippet 12-4: Statistical Drift Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet 12-4: Data Drift Detection\n",
    "\n",
    "class DriftDetector:\n",
    "    \"\"\"Detect data drift and concept drift in ML pipelines.\"\"\"\n",
    "    \n",
    "    def __init__(self, reference_df: pd.DataFrame, numeric_columns: List[str]):\n",
    "        self.reference_df = reference_df\n",
    "        self.numeric_columns = numeric_columns\n",
    "        self.reference_stats = self._compute_reference_stats()\n",
    "    \n",
    "    def _compute_reference_stats(self) -> Dict:\n",
    "        \"\"\"Compute reference statistics for comparison.\"\"\"\n",
    "        stats = {}\n",
    "        for col in self.numeric_columns:\n",
    "            if col in self.reference_df.columns:\n",
    "                data = self.reference_df[col].dropna()\n",
    "                stats[col] = {\n",
    "                    'mean': data.mean(),\n",
    "                    'std': data.std(),\n",
    "                    'median': data.median(),\n",
    "                    'q25': data.quantile(0.25),\n",
    "                    'q75': data.quantile(0.75)\n",
    "                }\n",
    "        return stats\n",
    "    \n",
    "    def calculate_psi(self, reference: pd.Series, current: pd.Series, \n",
    "                     n_bins: int = 10) -> float:\n",
    "        \"\"\"Calculate Population Stability Index (PSI).\"\"\"\n",
    "        # Create bins from reference data\n",
    "        min_val = min(reference.min(), current.min())\n",
    "        max_val = max(reference.max(), current.max())\n",
    "        bins = np.linspace(min_val, max_val, n_bins + 1)\n",
    "        \n",
    "        # Calculate proportions\n",
    "        ref_counts, _ = np.histogram(reference, bins=bins)\n",
    "        curr_counts, _ = np.histogram(current, bins=bins)\n",
    "        \n",
    "        # Add small value to avoid division by zero\n",
    "        ref_pct = (ref_counts + 0.001) / (len(reference) + 0.001 * n_bins)\n",
    "        curr_pct = (curr_counts + 0.001) / (len(current) + 0.001 * n_bins)\n",
    "        \n",
    "        # Calculate PSI\n",
    "        psi = np.sum((curr_pct - ref_pct) * np.log(curr_pct / ref_pct))\n",
    "        return psi\n",
    "    \n",
    "    def detect_data_drift(self, current_df: pd.DataFrame, \n",
    "                         psi_threshold: float = 0.2,\n",
    "                         ks_alpha: float = 0.05) -> Dict:\n",
    "        \"\"\"Detect data drift using multiple statistical tests.\"\"\"\n",
    "        drift_report = {\n",
    "            'drift_detected': False,\n",
    "            'feature_results': {},\n",
    "            'alerts': []\n",
    "        }\n",
    "        \n",
    "        for col in self.numeric_columns:\n",
    "            if col not in current_df.columns:\n",
    "                continue\n",
    "            \n",
    "            ref_data = self.reference_df[col].dropna()\n",
    "            curr_data = current_df[col].dropna()\n",
    "            \n",
    "            # PSI test\n",
    "            psi = self.calculate_psi(ref_data, curr_data)\n",
    "            \n",
    "            # Kolmogorov-Smirnov test\n",
    "            ks_stat, ks_pvalue = stats.ks_2samp(ref_data, curr_data)\n",
    "            \n",
    "            # Mean shift\n",
    "            ref_mean = ref_data.mean()\n",
    "            curr_mean = curr_data.mean()\n",
    "            mean_shift = (curr_mean - ref_mean) / ref_mean if ref_mean != 0 else 0\n",
    "            \n",
    "            feature_result = {\n",
    "                'psi': psi,\n",
    "                'psi_drift': psi > psi_threshold,\n",
    "                'ks_statistic': ks_stat,\n",
    "                'ks_pvalue': ks_pvalue,\n",
    "                'ks_drift': ks_pvalue < ks_alpha,\n",
    "                'mean_shift_pct': mean_shift * 100\n",
    "            }\n",
    "            \n",
    "            drift_report['feature_results'][col] = feature_result\n",
    "            \n",
    "            # Generate alerts\n",
    "            if psi > psi_threshold:\n",
    "                drift_report['drift_detected'] = True\n",
    "                severity = 'HIGH' if psi > 0.25 else 'MEDIUM'\n",
    "                drift_report['alerts'].append(\n",
    "                    f\"{severity}: {col} PSI={psi:.3f} exceeds threshold {psi_threshold}\"\n",
    "                )\n",
    "            \n",
    "            if ks_pvalue < ks_alpha:\n",
    "                drift_report['drift_detected'] = True\n",
    "                drift_report['alerts'].append(\n",
    "                    f\"STATISTICAL: {col} KS test p-value={ks_pvalue:.4f} < {ks_alpha}\"\n",
    "                )\n",
    "        \n",
    "        return drift_report\n",
    "\n",
    "\n",
    "# Create reference data (first half) and current data (second half with drift)\n",
    "n_half = len(customers_df) // 2\n",
    "reference_customers = customers_df.iloc[:n_half].copy()\n",
    "current_customers = customers_df.iloc[n_half:].copy()\n",
    "\n",
    "# Introduce artificial drift in current data\n",
    "current_customers['lifetime_value'] = current_customers['lifetime_value'] * 1.3 + 100\n",
    "\n",
    "# Detect drift\n",
    "detector = DriftDetector(\n",
    "    reference_customers, \n",
    "    numeric_columns=['lifetime_value']\n",
    ")\n",
    "\n",
    "drift_report = detector.detect_data_drift(current_customers)\n",
    "\n",
    "print(\"Drift Detection Report:\")\n",
    "print(f\"Drift Detected: {drift_report['drift_detected']}\")\n",
    "print(f\"\\nFeature Results:\")\n",
    "for feature, results in drift_report['feature_results'].items():\n",
    "    print(f\"  {feature}:\")\n",
    "    print(f\"    PSI: {results['psi']:.4f} (drift: {results['psi_drift']})\")\n",
    "    print(f\"    KS p-value: {results['ks_pvalue']:.4f} (drift: {results['ks_drift']})\")\n",
    "    print(f\"    Mean shift: {results['mean_shift_pct']:.1f}%\")\n",
    "print(f\"\\nAlerts: {drift_report['alerts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.7: Late-Arriving Data Handling\n",
    "\n",
    "Incremental pipelines must handle records that arrive after their logical time window has been processed.\n",
    "\n",
    "### Snippet 12-5: Late-Arriving Data Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet 12-5: Late-Arriving Data Handling\n",
    "\n",
    "class LateDataHandler:\n",
    "    \"\"\"Handle late-arriving data in incremental pipelines.\"\"\"\n",
    "    \n",
    "    def __init__(self, late_arrival_lookback_days: int = 3):\n",
    "        self.late_arrival_lookback_days = late_arrival_lookback_days\n",
    "    \n",
    "    def process_with_late_handling(self, df: pd.DataFrame, \n",
    "                                   event_date_col: str,\n",
    "                                   ingestion_date_col: str,\n",
    "                                   processing_date: datetime) -> Dict:\n",
    "        \"\"\"\n",
    "        Process data while accounting for late arrivals.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            event_date_col: Column with event timestamp\n",
    "            ingestion_date_col: Column with ingestion timestamp\n",
    "            processing_date: The date we're processing for\n",
    "        \n",
    "        Returns:\n",
    "            Dict with primary and late-arriving records\n",
    "        \"\"\"\n",
    "        # Define windows\n",
    "        primary_start = processing_date - timedelta(days=1)\n",
    "        primary_end = processing_date\n",
    "        late_window_start = processing_date - timedelta(days=self.late_arrival_lookback_days)\n",
    "        \n",
    "        # Ensure datetime columns\n",
    "        df[event_date_col] = pd.to_datetime(df[event_date_col])\n",
    "        df[ingestion_date_col] = pd.to_datetime(df[ingestion_date_col])\n",
    "        \n",
    "        # Primary data: events from yesterday\n",
    "        primary_mask = (\n",
    "            (df[event_date_col] >= primary_start) &\n",
    "            (df[event_date_col] < primary_end)\n",
    "        )\n",
    "        \n",
    "        # Late arrivals: old events that were just ingested\n",
    "        late_mask = (\n",
    "            (df[event_date_col] >= late_window_start) &\n",
    "            (df[event_date_col] < primary_start) &\n",
    "            (df[ingestion_date_col] >= primary_start)  # Recently ingested\n",
    "        )\n",
    "        \n",
    "        primary_records = df[primary_mask]\n",
    "        late_records = df[late_mask]\n",
    "        \n",
    "        return {\n",
    "            'primary_records': primary_records,\n",
    "            'late_records': late_records,\n",
    "            'primary_count': len(primary_records),\n",
    "            'late_count': len(late_records),\n",
    "            'processing_date': processing_date\n",
    "        }\n",
    "\n",
    "\n",
    "# Simulate late-arriving data\n",
    "test_df = transactions_df.copy()\n",
    "test_df['ingestion_timestamp'] = test_df['transaction_timestamp'] + pd.to_timedelta(\n",
    "    np.random.choice([0, 1, 2, 3], len(test_df), p=[0.9, 0.05, 0.03, 0.02]), unit='D'\n",
    ")\n",
    "\n",
    "handler = LateDataHandler(late_arrival_lookback_days=3)\n",
    "result = handler.process_with_late_handling(\n",
    "    test_df,\n",
    "    event_date_col='transaction_timestamp',\n",
    "    ingestion_date_col='ingestion_timestamp',\n",
    "    processing_date=pd.Timestamp('2024-06-15')\n",
    ")\n",
    "\n",
    "print(f\"Processing date: {result['processing_date']}\")\n",
    "print(f\"Primary records (yesterday): {result['primary_count']}\")\n",
    "print(f\"Late-arriving records: {result['late_count']}\")\n",
    "if result['late_count'] > 0:\n",
    "    print(f\"\\nLate records sample:\")\n",
    "    print(result['late_records'][['transaction_timestamp', 'ingestion_timestamp']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.8: Airflow DAG Pattern (Reference)\n",
    "\n",
    "This section shows the TaskFlow API pattern for modern Airflow DAGs. This code would run in an Airflow environment.\n",
    "\n",
    "### Snippet 12-6: Modern Airflow DAG Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet 12-6: Modern Airflow DAG Pattern (Reference Implementation)\n",
    "# This code demonstrates the pattern - it would run in an Airflow environment\n",
    "\n",
    "DAG_CODE = '''\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'ml-team',\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'retry_exponential_backoff': True,\n",
    "}\n",
    "\n",
    "@dag(\n",
    "    dag_id='automl_data_pipeline',\n",
    "    default_args=default_args,\n",
    "    schedule='0 2 * * *',  # Daily at 2 AM\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['automl', 'data-pipeline'],\n",
    ")\n",
    "def automl_pipeline():\n",
    "    \"\"\"Modern TaskFlow DAG for AutoML data pipeline.\"\"\"\n",
    "    \n",
    "    @task\n",
    "    def extract_data(**context):\n",
    "        \"\"\"Extract data with point-in-time correctness.\"\"\"\n",
    "        execution_date = context['execution_date']\n",
    "        # Extract logic here\n",
    "        return {'path': '/data/extracted.parquet', 'count': 1000}\n",
    "    \n",
    "    @task\n",
    "    def validate_data(extract_result, **context):\n",
    "        \"\"\"Validate data quality.\"\"\"\n",
    "        # Validation logic here\n",
    "        return {'valid': True, 'path': extract_result['path']}\n",
    "    \n",
    "    @task\n",
    "    def compute_features(validated_result, **context):\n",
    "        \"\"\"Compute features with temporal correctness.\"\"\"\n",
    "        # Feature engineering logic here\n",
    "        return {'features_path': '/data/features.parquet'}\n",
    "    \n",
    "    @task\n",
    "    def check_drift(features_result, **context):\n",
    "        \"\"\"Check for data drift.\"\"\"\n",
    "        # Drift detection logic here\n",
    "        return {'drift_detected': False}\n",
    "    \n",
    "    # Define flow using TaskFlow\n",
    "    extracted = extract_data()\n",
    "    validated = validate_data(extracted)\n",
    "    features = compute_features(validated)\n",
    "    drift_check = check_drift(features)\n",
    "\n",
    "# Instantiate\n",
    "dag = automl_pipeline()\n",
    "'''\n",
    "\n",
    "print(\"Modern Airflow DAG Pattern (TaskFlow API):\")\n",
    "print(\"=\" * 50)\n",
    "print(DAG_CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.9: Idempotent Operations\n",
    "\n",
    "Idempotency is critical for production pipelines - operations should produce identical results when run multiple times.\n",
    "\n",
    "### Snippet 12-7: Idempotent Write Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet 12-7: Idempotent Write Pattern\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "class IdempotentWriter:\n",
    "    \"\"\"Write data idempotently using atomic operations.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_parquet(df: pd.DataFrame, output_path: str, \n",
    "                     partition_key: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Write DataFrame to parquet idempotently.\n",
    "        \n",
    "        Uses atomic rename to ensure partial writes don't corrupt data.\n",
    "        Running this multiple times with same data produces same result.\n",
    "        \"\"\"\n",
    "        # Create temp file in same directory (for atomic rename)\n",
    "        output_dir = os.path.dirname(output_path) or '.'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        temp_path = f\"{output_path}.tmp.{os.getpid()}\"\n",
    "        \n",
    "        try:\n",
    "            # Write to temp location\n",
    "            df.to_parquet(temp_path, index=False)\n",
    "            \n",
    "            # Atomic rename (overwrites existing = idempotent)\n",
    "            shutil.move(temp_path, output_path)\n",
    "            \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Clean up temp file on failure\n",
    "            if os.path.exists(temp_path):\n",
    "                os.remove(temp_path)\n",
    "            raise e\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_partitioned(df: pd.DataFrame, base_path: str,\n",
    "                         partition_col: str, execution_date: datetime) -> str:\n",
    "        \"\"\"\n",
    "        Write with date partitioning for idempotent incremental processing.\n",
    "        \"\"\"\n",
    "        partition_value = execution_date.strftime('%Y-%m-%d')\n",
    "        partition_path = os.path.join(base_path, f\"dt={partition_value}\")\n",
    "        output_path = os.path.join(partition_path, \"data.parquet\")\n",
    "        \n",
    "        return IdempotentWriter.write_parquet(df, output_path)\n",
    "\n",
    "\n",
    "# Demonstrate idempotent write\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    # First write\n",
    "    path1 = IdempotentWriter.write_partitioned(\n",
    "        customers_df.head(100),\n",
    "        tmpdir,\n",
    "        partition_col='execution_date',\n",
    "        execution_date=datetime(2024, 6, 15)\n",
    "    )\n",
    "    print(f\"First write: {path1}\")\n",
    "    \n",
    "    # Second write (same data, same path = idempotent)\n",
    "    path2 = IdempotentWriter.write_partitioned(\n",
    "        customers_df.head(100),\n",
    "        tmpdir,\n",
    "        partition_col='execution_date',\n",
    "        execution_date=datetime(2024, 6, 15)\n",
    "    )\n",
    "    print(f\"Second write: {path2}\")\n",
    "    \n",
    "    # Verify same result\n",
    "    df1 = pd.read_parquet(path1)\n",
    "    df2 = pd.read_parquet(path2)\n",
    "    print(f\"\\nResults identical: {df1.equals(df2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.10: Summary\n",
    "\n",
    "This notebook demonstrated key patterns for building robust data pipelines that support AutoML:\n",
    "\n",
    "1. **Point-in-Time Correctness** - Always filter features to only use data available before prediction time\n",
    "\n",
    "2. **Hierarchical Validation** - Run cheap checks first, fail fast on obvious problems\n",
    "\n",
    "3. **Data Contracts** - Formalize expectations about data schema and quality\n",
    "\n",
    "4. **Drift Detection** - Monitor for both data drift (distributions) and concept drift (relationships)\n",
    "\n",
    "5. **Late-Arriving Data** - Handle records that arrive after their logical time window\n",
    "\n",
    "6. **Modern Airflow Patterns** - Use TaskFlow API and @dag decorator for cleaner code\n",
    "\n",
    "7. **Idempotent Operations** - Ensure operations produce identical results when re-run\n",
    "\n",
    "These patterns form the foundation of reliable AutoML data infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chapter 12 notebook complete!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"- Point-in-time correctness prevents data leakage\")\n",
    "print(\"- Hierarchical validation catches issues early and cheaply\")\n",
    "print(\"- Data contracts formalize data expectations\")\n",
    "print(\"- Drift detection enables proactive model maintenance\")\n",
    "print(\"- Idempotency ensures reliable pipeline operations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl-fresh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}