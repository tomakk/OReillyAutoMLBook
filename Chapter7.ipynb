{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Working with Tabular Data - Complete Code Examples\n",
    "\n",
    "This notebook contains all the code examples from Chapter 7. It provides a comprehensive guide to using AutoGluon's `TabularPredictor` for classification and regression tasks, covering everything from basic setup to advanced customization and a full project implementation.\n",
    "\n",
    "**AutoGluon Version: 1.5.0** (latest stable release as of this writing)\n",
    "\n",
    "### Contents:\n",
    "1. Environment Setup\n",
    "2. TabularPredictor Basics\n",
    "3. AutoGluon's Automatic Data Processing\n",
    "4. Advanced Customization\n",
    "5. Project: Titanic Survival Prediction\n",
    "6. Model Interpretability and SHAP Analysis\n",
    "7. Data Pipeline Consistency\n",
    "8. Monitoring and Maintaining Models in Production\n",
    "9. Summary and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Required installations before running this notebook:\n",
    "\n",
    "Lightweight (~500 MB, ~30 dependencies):\n",
    "    pip install autogluon.tabular\n",
    "\n",
    "Standard (~1.5 GB, ~60 dependencies) - Recommended:\n",
    "    pip install autogluon.tabular[all]\n",
    "\n",
    "Full (~5-8 GB, ~150+ dependencies):\n",
    "    pip install autogluon\n",
    "\n",
    "Additional packages for this notebook:\n",
    "    pip install pandas numpy matplotlib seaborn scikit-learn psutil shap\n",
    "\"\"\"\n",
    "\n",
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure output directory exists for saved figures\n",
    "os.makedirs('./ag_models', exist_ok=True)\n",
    "\n",
    "# Set random seed for AutoGluon reproducibility\n",
    "os.environ['AG_SEED'] = '42'\n",
    "\n",
    "# AutoGluon import\n",
    "try:\n",
    "    from autogluon.tabular import TabularPredictor\n",
    "    import autogluon.tabular as ag_tabular\n",
    "    print(f\"AutoGluon Tabular version: {ag_tabular.__version__} successfully imported!\")\n",
    "except ImportError:\n",
    "    print(\"Please install AutoGluon: pip install autogluon.tabular[all]\")\n",
    "\n",
    "# System check\n",
    "import psutil\n",
    "import platform\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"CPU Cores: {psutil.cpu_count()}\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "print(\"\\nNote: For tabular data, GPUs provide minimal benefit. Tree-based models run on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. TabularPredictor Basics - Adult/Census Income Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adult_dataset():\n",
    "    \"\"\"Load and prepare the Adult/Census Income dataset.\"\"\"\n",
    "    train_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "    test_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    "    columns = [\n",
    "        'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "        'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
    "        'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income'\n",
    "    ]\n",
    "    try:\n",
    "        print(\"Attempting to load Adult dataset from UCI repository...\")\n",
    "        train_data = pd.read_csv(train_url, names=columns, skipinitialspace=True, na_values='?')\n",
    "        test_data = pd.read_csv(test_url, names=columns, skipinitialspace=True, skiprows=1, na_values='?')\n",
    "        data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "        data['income'] = data['income'].str.replace('.', '', regex=False).str.strip()\n",
    "        print(f\"Adult dataset loaded successfully: {data.shape}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load real dataset ({e}). A local copy may be required.\")\n",
    "        return pd.DataFrame(columns=columns)\n",
    "\n",
    "adult_data = load_adult_dataset()\n",
    "if adult_data.empty:\n",
    "    raise RuntimeError(\n",
    "        \"Dataset could not be loaded. Please check your internet connection \"\n",
    "        \"or provide a local copy of the Adult/Census Income dataset.\"\n",
    "    )\n",
    "train_data, test_data = train_test_split(adult_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"Training samples: {len(train_data)}, Test samples: {len(test_data)}\")\n",
    "print(\"\\nTarget Distribution:\")\n",
    "print(train_data['income'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting basic AutoGluon training...\")\n",
    "\n",
    "# Initialize and train the TabularPredictor\n",
    "predictor = TabularPredictor(\n",
    "    label='income',\n",
    "    eval_metric='roc_auc',\n",
    "    path='./ag_models/adult_income'\n",
    ")\n",
    "\n",
    "# Fit the model with a time limit\n",
    "predictor.fit(train_data, time_limit=120)  # 2-minute time limit for a quick run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Predictor Output\n",
    "\n",
    "The leaderboard shows cross-validation performance. However, even cross-validation scores can overestimate true generalization performance, especially with small datasets. The final test set evaluation is what really matters for deployment decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model leaderboard\n",
    "print(\"Model Leaderboard:\")\n",
    "leaderboard = predictor.leaderboard(test_data, silent=True)\n",
    "print(leaderboard)\n",
    "\n",
    "# Display feature importance\n",
    "print(\"\\nFeature Importance:\")\n",
    "feature_importance = predictor.feature_importance(test_data)\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification in Detail\n",
    "\n",
    "**Metric selection guide for citizen data scientists:**\n",
    "- **Accuracy**: Use when all errors are equally bad\n",
    "- **ROC-AUC**: Use when you need to rank items by probability\n",
    "- **F1**: Use when you care about catching positive cases even at the cost of some false alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed evaluation metrics\n",
    "print(\"Detailed Evaluation Metrics:\")\n",
    "eval_metrics = predictor.evaluate(test_data, silent=True)\n",
    "for metric, value in eval_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Generate predictions and create a confusion matrix\n",
    "y_pred = predictor.predict(test_data)\n",
    "y_true = test_data['income']\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=predictor.class_labels, yticklabels=predictor.class_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig('./ag_models/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Calibration Note\n",
    "\n",
    "AutoGluon's probability estimates may not be well-calibrated out of the box. If you're using these probabilities for decision thresholds or risk scoring, consider applying calibration techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction probabilities example\n",
    "sample_data = test_data.head(5)\n",
    "\n",
    "# Basic predictions (class labels)\n",
    "predictions = predictor.predict(sample_data)\n",
    "print(\"Class predictions:\")\n",
    "print(predictions.values)\n",
    "\n",
    "# Prediction probabilities for confidence scoring\n",
    "probabilities = predictor.predict_proba(sample_data)\n",
    "print(\"\\nPrediction probabilities:\")\n",
    "print(probabilities)\n",
    "\n",
    "# Note: For production use with thresholds, consider Platt scaling or isotonic regression\n",
    "# from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing data for Multi-Class Classification (predicting 'workclass')...\")\n",
    "multiclass_data = adult_data.dropna(subset=['workclass']).copy()\n",
    "\n",
    "mc_train, mc_test = train_test_split(multiclass_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a multi-class predictor\n",
    "mc_predictor = TabularPredictor(\n",
    "    label='workclass',\n",
    "    eval_metric='accuracy',\n",
    "    path='./ag_models/multi_class'\n",
    ").fit(mc_train, time_limit=120)\n",
    "\n",
    "# Evaluate the multi-class model\n",
    "mc_performance = mc_predictor.evaluate(mc_test)\n",
    "print(f\"\\nMulti-class model performance: {mc_performance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Example\n",
    "\n",
    "**RMSE Note**: RMSE is the square root of the average squared error, making it more sensitive to large errors than MAE. If you have outliers, RMSE will be pulled higher. When outliers matter, use RMSE; when you want robustness to outliers, prefer MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing data for Regression (predicting 'hours_per_week')...\")\n",
    "regression_data = adult_data.dropna(subset=['hours_per_week']).copy()\n",
    "reg_train, reg_test = train_test_split(regression_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a regression predictor\n",
    "reg_predictor = TabularPredictor(\n",
    "    label='hours_per_week',\n",
    "    problem_type='regression',\n",
    "    eval_metric='root_mean_squared_error',\n",
    "    path='./ag_models/regression'\n",
    ").fit(reg_train, time_limit=120)\n",
    "\n",
    "# Evaluate and plot regression results\n",
    "reg_metrics = reg_predictor.evaluate(reg_test)\n",
    "print(f\"\\nRegression Metrics: {reg_metrics}\")\n",
    "\n",
    "reg_predictions = reg_predictor.predict(reg_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(reg_test['hours_per_week'], reg_predictions, alpha=0.3)\n",
    "plt.plot([0, 100], [0, 100], 'r--')\n",
    "plt.xlabel('Actual Hours per Week')\n",
    "plt.ylabel('Predicted Hours per Week')\n",
    "plt.title('Regression: Predicted vs. Actual')\n",
    "plt.grid(True)\n",
    "plt.savefig('./ag_models/regression_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. AutoGluon's Automatic Data Processing\n",
    "\n",
    "AutoGluon handles most data processing automatically. This section demonstrates its capabilities.\n",
    "\n",
    "**Advanced insight**: The missingness pattern itself is often informative. In healthcare data, a missing lab test might indicate the doctor didn't think it was necessary. Consider adding explicit \"is_missing\" indicator features before training if missingness carries meaning in your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with various data types and missing values to demonstrate automatic handling\n",
    "processing_demo_data = adult_data.sample(1000, random_state=42).copy()\n",
    "\n",
    "print(\"Original Missing Values:\")\n",
    "print(processing_demo_data.isnull().sum()[processing_demo_data.isnull().sum() > 0])\n",
    "\n",
    "# Add datetime features\n",
    "processing_demo_data['hire_date'] = pd.to_datetime('2022-01-01') - pd.to_timedelta(\n",
    "    processing_demo_data['age'] * 365, 'd'\n",
    ")\n",
    "\n",
    "print(\"\\nTraining a model on this mixed-type, missing-value data...\")\n",
    "# AutoGluon will automatically detect data types, handle missing values, and engineer features\n",
    "processing_predictor = TabularPredictor(label='income', path='./ag_models/data_processing_demo')\n",
    "processing_predictor.fit(processing_demo_data, time_limit=60, verbosity=2)\n",
    "\n",
    "print(\"\\nAutoGluon automatically handled:\")\n",
    "print(\"- Missing values in 'workclass' and 'occupation'\")\n",
    "print(\"- High-cardinality categorical features like 'native_country'\")\n",
    "print(\"- Datetime feature 'hire_date' by extracting useful components (year, month, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Advanced Customization\n",
    "\n",
    "**Warning**: Overly conservative hyperparameter settings can underfit, especially with smaller datasets. Match model complexity to your data size and signal-to-noise ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Demonstrating Advanced Customization...\")\n",
    "\n",
    "# 1. Custom Hyperparameters for specific models\n",
    "hyperparameters = {\n",
    "    'GBM': [\n",
    "        {'num_boost_round': 10000, 'learning_rate': 0.05, 'num_leaves': 26},\n",
    "        {'num_boost_round': 5000, 'learning_rate': 0.1, 'num_leaves': 32},\n",
    "    ],\n",
    "    'RF': [\n",
    "        {'n_estimators': 300, 'max_depth': 15, 'n_jobs': -1},\n",
    "    ],\n",
    "}\n",
    "\n",
    "# 2. Custom Ensemble Configuration\n",
    "custom_predictor = TabularPredictor(\n",
    "    label='income',\n",
    "    path='./ag_models/custom_config'\n",
    ").fit(\n",
    "    train_data,\n",
    "    time_limit=180,\n",
    "    hyperparameters=hyperparameters,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=1,\n",
    "    num_stack_levels=1,\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "print(\"\\nCustom Model Leaderboard:\")\n",
    "custom_leaderboard = custom_predictor.leaderboard(test_data, silent=True)\n",
    "print(custom_leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Project: Titanic Survival Prediction\n",
    "\n",
    "**Note on educational datasets**: The Titanic dataset is perfect for learning, but it's been heavily used in ML education for decades. Performance numbers here may be optimistic compared to truly novel problems where you don't have accumulated community wisdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Titanic Survival Prediction Project...\")\n",
    "\n",
    "# Load Titanic dataset from seaborn (consistent with chapter text)\n",
    "def load_titanic_dataset():\n",
    "    \"\"\"Load Titanic dataset with comprehensive preprocessing\"\"\"\n",
    "    try:\n",
    "        titanic = sns.load_dataset('titanic')\n",
    "        \n",
    "        # Drop columns that would cause data leakage or are redundant:\n",
    "        # - 'alive': string version of 'survived' (direct leakage!)\n",
    "        # - 'class': redundant with 'pclass'\n",
    "        # - 'who': derived from sex/age, could leak information\n",
    "        # - 'adult_male': derived from sex/age\n",
    "        leaky_columns = ['alive', 'class', 'who', 'adult_male']\n",
    "        titanic = titanic.drop(columns=[c for c in leaky_columns if c in titanic.columns])\n",
    "        \n",
    "        print(f\"Titanic dataset loaded: {titanic.shape}\")\n",
    "        print(f\"Columns: {titanic.columns.tolist()}\")\n",
    "        return titanic\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load Titanic dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "titanic_data = load_titanic_dataset()\n",
    "\n",
    "print(f\"\\nSurvival rate: {titanic_data['survived'].mean():.2%}\")\n",
    "print(\"\\nSurvival by key factors:\")\n",
    "print(\"By Sex:\")\n",
    "print(titanic_data.groupby('sex')['survived'].mean())\n",
    "print(\"\\nBy Class:\")\n",
    "print(titanic_data.groupby('pclass')['survived'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Feature Engineering for Titanic\n",
    "\n",
    "**IMPORTANT**: This exact function must be used in both training and inference. Any differences between training and serving feature engineering will cause silent model degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_titanic_features(df):\n",
    "    \"\"\"Engineer domain-specific features for Titanic dataset\n",
    "    \n",
    "    IMPORTANT: This exact function must be used in both training and inference.\n",
    "    Any differences between training and serving feature engineering will cause\n",
    "    silent model degradation. Consider packaging this function with your model\n",
    "    or using a feature store to ensure consistency.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Family size features\n",
    "    if 'sibsp' in df.columns and 'parch' in df.columns:\n",
    "        df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
    "        df['is_alone'] = (df['family_size'] == 1).astype(int)\n",
    "    \n",
    "    # Age groups\n",
    "    if 'age' in df.columns:\n",
    "        df['age_group'] = pd.cut(\n",
    "            df['age'],\n",
    "            bins=[0, 12, 20, 40, 60, 100],\n",
    "            labels=['child', 'teen', 'adult', 'middle_age', 'senior']\n",
    "        )\n",
    "    \n",
    "    # Fare per person and class interactions\n",
    "    if 'fare' in df.columns and 'family_size' in df.columns:\n",
    "        df['fare_per_person'] = df['fare'] / df['family_size']\n",
    "    \n",
    "    if 'pclass' in df.columns and 'sex' in df.columns:\n",
    "        df['class_sex'] = df['pclass'].astype(str) + '_' + df['sex']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Split data\n",
    "titanic_train = titanic_data.sample(frac=0.8, random_state=42)\n",
    "titanic_test = titanic_data.drop(titanic_train.index)\n",
    "\n",
    "print(f\"Training samples: {len(titanic_train)}, Test samples: {len(titanic_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model (no feature engineering)\n",
    "print(\"Training baseline model...\")\n",
    "titanic_predictor_baseline = TabularPredictor(\n",
    "    label='survived',\n",
    "    eval_metric='roc_auc',\n",
    "    path='./ag_models/titanic_baseline'\n",
    ")\n",
    "titanic_predictor_baseline.fit(titanic_train, time_limit=120)\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_eval = titanic_predictor_baseline.evaluate(titanic_test, silent=True)\n",
    "print(f\"\\nBaseline Titanic ROC-AUC: {baseline_eval['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train improved model with feature engineering\n",
    "print(\"Training improved model with feature engineering...\")\n",
    "\n",
    "titanic_train_eng = engineer_titanic_features(titanic_train)\n",
    "titanic_test_eng = engineer_titanic_features(titanic_test)\n",
    "\n",
    "titanic_predictor_improved = TabularPredictor(\n",
    "    label='survived',\n",
    "    eval_metric='roc_auc',\n",
    "    path='./ag_models/titanic_improved'\n",
    ")\n",
    "titanic_predictor_improved.fit(titanic_train_eng, time_limit=120)\n",
    "\n",
    "# Evaluate improved model\n",
    "improved_eval = titanic_predictor_improved.evaluate(titanic_test_eng, silent=True)\n",
    "print(f\"\\nImproved Titanic ROC-AUC: {improved_eval['roc_auc']:.4f}\")\n",
    "print(f\"Improvement: {improved_eval['roc_auc'] - baseline_eval['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Interpretability and SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis (Figure 7-6)\n",
    "print(\"Feature Importance Analysis\")\n",
    "importance = titanic_predictor_improved.feature_importance(titanic_test_eng)\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance.head(10))\n",
    "\n",
    "# Create Figure 7-6: Feature importance visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_importance = importance.head(10)\n",
    "colors = sns.color_palette(\"Blues_r\", len(top_importance))\n",
    "bars = plt.barh(range(len(top_importance)), top_importance['importance'].values, color=colors)\n",
    "\n",
    "plt.yticks(range(len(top_importance)), top_importance.index)\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Feature Importance Analysis\\nRelative Contribution to Survival Predictions', fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, top_importance['importance'].values)):\n",
    "    plt.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2,\n",
    "             f'{val:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('./ag_models/figure_7_6_feature_importance.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(\"\\nFigure 7-6 saved to: ./ag_models/figure_7_6_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model leaderboard comparison\n",
    "print(\"\\nModel Performance Comparison (Leaderboard):\")\n",
    "leaderboard = titanic_predictor_improved.leaderboard(titanic_test_eng, silent=True)\n",
    "print(leaderboard.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Analysis for Individual Predictions\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) provides both global and local interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis (requires shap package)\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    # Get the best single model (not ensemble) for SHAP analysis\n",
    "    best_model = leaderboard[~leaderboard['model'].str.contains('Ensemble')].iloc[0]['model']\n",
    "    print(f\"Using model for SHAP: {best_model}\")\n",
    "    \n",
    "    # Create explainer\n",
    "    # Note: For tree-based models, use TreeExplainer for efficiency\n",
    "    sample_for_shap = titanic_test_eng.drop('survived', axis=1).head(100)\n",
    "    \n",
    "    # Get predictions for SHAP baseline\n",
    "    predictions = titanic_predictor_improved.predict_proba(sample_for_shap, model=best_model)\n",
    "    \n",
    "    # Compute SHAP values\n",
    "    try:\n",
    "        # Try TreeExplainer first (fast, works with tree-based models)\n",
    "        internal_model = predictor._learner.trainer.load_model(best_model)\n",
    "        explainer = shap.TreeExplainer(internal_model)\n",
    "        print(\"Using TreeExplainer (fast path for tree-based models)\")\n",
    "    except Exception:\n",
    "        # Fall back to KernelExplainer (model-agnostic but slower)\n",
    "        print(\"TreeExplainer not compatible; falling back to KernelExplainer (this may take a minute)...\")\n",
    "        explainer = shap.KernelExplainer(\n",
    "            lambda x: titanic_predictor_improved.predict_proba(\n",
    "                pd.DataFrame(x, columns=sample_for_shap.columns)\n",
    "            ).values,\n",
    "            shap.sample(sample_for_shap, 50)\n",
    "        )\n",
    "    \n",
    "    shap_values = explainer(sample_for_shap)\n",
    "    \n",
    "    # Summary plot showing feature impact across all predictions\n",
    "    shap.summary_plot(shap_values, sample_for_shap, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nSHAP analysis complete.\")\n",
    "    print(\"You can also create individual prediction explanations with:\")\n",
    "    print(\"  shap.waterfall_plot(shap_values[0])\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"SHAP not installed. Install with: pip install shap\")\n",
    "except Exception as e:\n",
    "    print(f\"SHAP analysis encountered an issue: {e}\")\n",
    "    print(\"This is normal - SHAP requires specific model access patterns.\")\n",
    "    print(\"Feature importance (shown above) provides an alternative view of feature contributions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production-Ready Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_passenger_survival(passenger_df, predictor=None):\n",
    "    \"\"\"Production-ready prediction function for Titanic survival\n",
    "    \n",
    "    For production deployment, includes:\n",
    "    - Input validation (expected columns, value ranges)\n",
    "    - Prediction logging for monitoring\n",
    "    - Structured error responses for debugging\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    if predictor is None:\n",
    "        predictor = titanic_predictor_improved\n",
    "    \n",
    "    try:\n",
    "        # Input validation\n",
    "        required_cols = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
    "        available_cols = [c for c in required_cols if c in passenger_df.columns]\n",
    "        missing_cols = [c for c in required_cols if c not in passenger_df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            logger.warning(f\"Missing columns (will use defaults): {missing_cols}\")\n",
    "        \n",
    "        # Apply same feature engineering as training\n",
    "        engineered_df = engineer_titanic_features(passenger_df)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = predictor.predict(engineered_df)\n",
    "        probability = predictor.predict_proba(engineered_df)\n",
    "        \n",
    "        # Get survival probability (class 1)\n",
    "        if probability.shape[1] > 1:\n",
    "            survival_prob = float(probability.iloc[0, 1])\n",
    "        else:\n",
    "            survival_prob = float(probability.iloc[0, 0])\n",
    "        \n",
    "        result = {\n",
    "            'prediction': int(prediction.iloc[0]),\n",
    "            'survival_probability': survival_prob,\n",
    "            'confidence': 'high' if survival_prob > 0.8 or survival_prob < 0.2 else 'medium'\n",
    "        }\n",
    "        \n",
    "        # Log prediction for monitoring\n",
    "        logger.info(f\"Prediction made: {result}\")\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction failed: {str(e)}\", exc_info=True)\n",
    "        return {'error': str(e), 'error_type': type(e).__name__}\n",
    "\n",
    "# Test the prediction function\n",
    "sample_passenger = pd.DataFrame({\n",
    "    'pclass': [1],\n",
    "    'sex': ['female'],\n",
    "    'age': [29.0],\n",
    "    'sibsp': [0],\n",
    "    'parch': [0],\n",
    "    'fare': [211.34],\n",
    "    'embarked': ['S'],\n",
    "    'deck': ['C'],\n",
    "    'embark_town': ['Southampton'],\n",
    "    'alone': [True]\n",
    "})\n",
    "\n",
    "result = predict_passenger_survival(sample_passenger)\n",
    "print(f\"\\nSample prediction for 1st class female passenger:\")\n",
    "print(f\"  Prediction: {'Survived' if result.get('prediction') == 1 else 'Did not survive'}\")\n",
    "print(f\"  Survival probability: {result.get('survival_probability', 0):.2%}\")\n",
    "print(f\"  Confidence: {result.get('confidence', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Data Pipeline Consistency\n",
    "\n",
    "A critical but often overlooked aspect of production ML is ensuring your data pipeline is identical in training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating pipeline consistency issues\n",
    "\n",
    "print(\"Data Pipeline Consistency Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate training statistics (these should be SAVED)\n",
    "training_stats = {\n",
    "    'age_median': titanic_train['age'].median(),\n",
    "    'fare_median': titanic_train['fare'].median(),\n",
    "    'embarked_mode': titanic_train['embarked'].mode()[0] if 'embarked' in titanic_train.columns else 'S'\n",
    "}\n",
    "\n",
    "print(\"\\nTraining statistics (save these for inference):\")\n",
    "for stat, value in training_stats.items():\n",
    "    print(f\"  {stat}: {value}\")\n",
    "\n",
    "# WRONG: Recalculating statistics on inference data\n",
    "inference_stats_wrong = {\n",
    "    'age_median': titanic_test['age'].median(),\n",
    "    'fare_median': titanic_test['fare'].median(),\n",
    "}\n",
    "\n",
    "print(\"\\nWRONG - Inference statistics (would cause data leakage):\")\n",
    "for stat, value in inference_stats_wrong.items():\n",
    "    print(f\"  {stat}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Common pipeline bugs to avoid:\")\n",
    "print(\"- Recalculating statistics (mean, median) on inference data\")\n",
    "print(\"- Different order of feature engineering steps\")\n",
    "print(\"- Missing edge case handling not present in training\")\n",
    "print(\"- Version mismatches in preprocessing libraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Saving and loading preprocessing statistics\n",
    "import json\n",
    "\n",
    "def save_preprocessing_stats(stats, filepath):\n",
    "    \"\"\"Save preprocessing statistics for use during inference\"\"\"\n",
    "    # Convert numpy types to Python types for JSON serialization\n",
    "    serializable_stats = {}\n",
    "    for k, v in stats.items():\n",
    "        if hasattr(v, 'item'):\n",
    "            serializable_stats[k] = v.item()\n",
    "        else:\n",
    "            serializable_stats[k] = v\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(serializable_stats, f, indent=2)\n",
    "    print(f\"Saved preprocessing stats to {filepath}\")\n",
    "\n",
    "def load_preprocessing_stats(filepath):\n",
    "    \"\"\"Load preprocessing statistics for inference\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Save the training statistics\n",
    "save_preprocessing_stats(training_stats, './ag_models/preprocessing_stats.json')\n",
    "\n",
    "# In production, load and use these\n",
    "loaded_stats = load_preprocessing_stats('./ag_models/preprocessing_stats.json')\n",
    "print(f\"\\nLoaded stats: {loaded_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Monitoring and Maintaining Models in Production\n",
    "\n",
    "**Model drift is inevitable.** Data distributions change, user behavior evolves, business rules shift. A model trained in 2024 may perform poorly by 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring example: Track prediction distributions\n",
    "\n",
    "def create_monitoring_baseline(predictor, reference_data):\n",
    "    \"\"\"Create baseline statistics for monitoring\"\"\"\n",
    "    predictions = predictor.predict(reference_data)\n",
    "    probabilities = predictor.predict_proba(reference_data)\n",
    "    \n",
    "    baseline = {\n",
    "        'prediction_distribution': predictions.value_counts(normalize=True).to_dict(),\n",
    "        'mean_probability_class_1': float(probabilities.iloc[:, 1].mean()) if probabilities.shape[1] > 1 else float(probabilities.iloc[:, 0].mean()),\n",
    "        'std_probability_class_1': float(probabilities.iloc[:, 1].std()) if probabilities.shape[1] > 1 else float(probabilities.iloc[:, 0].std()),\n",
    "        'n_samples': len(reference_data)\n",
    "    }\n",
    "    return baseline\n",
    "\n",
    "def check_for_drift(predictor, new_data, baseline, threshold=0.1):\n",
    "    \"\"\"Check if new predictions differ significantly from baseline\"\"\"\n",
    "    new_predictions = predictor.predict(new_data)\n",
    "    new_probabilities = predictor.predict_proba(new_data)\n",
    "    \n",
    "    new_mean = float(new_probabilities.iloc[:, 1].mean()) if new_probabilities.shape[1] > 1 else float(new_probabilities.iloc[:, 0].mean())\n",
    "    \n",
    "    drift_score = abs(new_mean - baseline['mean_probability_class_1'])\n",
    "    \n",
    "    alerts = []\n",
    "    if drift_score > threshold:\n",
    "        alerts.append(f\"ALERT: Prediction mean shifted by {drift_score:.3f} (threshold: {threshold})\")\n",
    "    \n",
    "    return {\n",
    "        'drift_score': drift_score,\n",
    "        'new_mean': new_mean,\n",
    "        'baseline_mean': baseline['mean_probability_class_1'],\n",
    "        'alerts': alerts\n",
    "    }\n",
    "\n",
    "# Create monitoring baseline from training data\n",
    "monitoring_baseline = create_monitoring_baseline(titanic_predictor_improved, titanic_train_eng)\n",
    "print(\"Monitoring Baseline Created:\")\n",
    "print(f\"  Prediction distribution: {monitoring_baseline['prediction_distribution']}\")\n",
    "print(f\"  Mean survival probability: {monitoring_baseline['mean_probability_class_1']:.3f}\")\n",
    "\n",
    "# Check test data for drift (simulating production monitoring)\n",
    "drift_check = check_for_drift(titanic_predictor_improved, titanic_test_eng, monitoring_baseline)\n",
    "print(f\"\\nDrift Check Results:\")\n",
    "print(f\"  Drift score: {drift_check['drift_score']:.3f}\")\n",
    "print(f\"  New mean: {drift_check['new_mean']:.3f}, Baseline: {drift_check['baseline_mean']:.3f}\")\n",
    "if drift_check['alerts']:\n",
    "    for alert in drift_check['alerts']:\n",
    "        print(f\"  {alert}\")\n",
    "else:\n",
    "    print(\"  No drift alerts - predictions within expected range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distribution monitoring\n",
    "\n",
    "def monitor_feature_distributions(train_data, new_data, features_to_monitor):\n",
    "    \"\"\"Compare feature distributions between training and new data\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for feature in features_to_monitor:\n",
    "        if feature not in train_data.columns or feature not in new_data.columns:\n",
    "            continue\n",
    "            \n",
    "        if train_data[feature].dtype in ['int64', 'float64']:\n",
    "            # Numerical feature\n",
    "            train_mean = train_data[feature].mean()\n",
    "            new_mean = new_data[feature].mean()\n",
    "            train_std = train_data[feature].std()\n",
    "            \n",
    "            # Z-score of the shift\n",
    "            if train_std > 0:\n",
    "                shift_z = abs(new_mean - train_mean) / train_std\n",
    "            else:\n",
    "                shift_z = 0\n",
    "            \n",
    "            results[feature] = {\n",
    "                'type': 'numerical',\n",
    "                'train_mean': train_mean,\n",
    "                'new_mean': new_mean,\n",
    "                'shift_z_score': shift_z,\n",
    "                'alert': shift_z > 2  # Alert if shift > 2 standard deviations\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Monitor key features\n",
    "features_to_monitor = ['age', 'fare', 'sibsp', 'parch']\n",
    "feature_drift = monitor_feature_distributions(titanic_train, titanic_test, features_to_monitor)\n",
    "\n",
    "print(\"Feature Distribution Monitoring:\")\n",
    "print(\"=\" * 50)\n",
    "for feature, stats in feature_drift.items():\n",
    "    alert_marker = \"[!]\" if stats['alert'] else \"[OK]\"\n",
    "    print(f\"{alert_marker} {feature}:\")\n",
    "    print(f\"    Train mean: {stats['train_mean']:.2f}, New mean: {stats['new_mean']:.2f}\")\n",
    "    print(f\"    Shift (z-score): {stats['shift_z_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Tools Integration\n",
    "\n",
    "While AutoGluon doesn't include built-in monitoring, you can integrate with:\n",
    "- **MLflow**: Track experiments, model versions, deployment metadata\n",
    "- **Weights & Biases**: Visualize training runs and performance\n",
    "- **Amazon SageMaker Model Monitor**: Native integration with AutoGluon on AWS\n",
    "- **Custom dashboards**: Grafana dashboards tracking prediction statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Logging to MLflow (if installed)\n",
    "try:\n",
    "    import mlflow\n",
    "    \n",
    "    # Log model metrics\n",
    "    with mlflow.start_run(run_name=\"titanic_autogluon\"):\n",
    "        mlflow.log_params({\n",
    "            'model_type': 'AutoGluon TabularPredictor',\n",
    "            'autogluon_version': ag_tabular.__version__,\n",
    "            'training_samples': len(titanic_train)\n",
    "        })\n",
    "        mlflow.log_metrics({\n",
    "            'baseline_roc_auc': baseline_eval['roc_auc'],\n",
    "            'improved_roc_auc': improved_eval['roc_auc']\n",
    "        })\n",
    "        print(\"Logged to MLflow successfully\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"MLflow not installed. Install with: pip install mlflow\")\n",
    "except Exception as e:\n",
    "    print(f\"MLflow logging skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary and Best Practices\n",
    "\n",
    "This notebook demonstrated the power and simplicity of AutoGluon 1.5.0 for tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"CHAPTER 7 SUMMARY - Working with Tabular Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. AUTOGLUON VERSION: 1.5.0\")\n",
    "\n",
    "print(\"\\n2. KEY RESULTS:\")\n",
    "print(f\"   Adult Income Dataset (Binary Classification):\")\n",
    "print(f\"   - Trained in ~2 minutes\")\n",
    "print(f\"   - Multiple models evaluated automatically\")\n",
    "\n",
    "print(f\"\\n   Titanic Dataset:\")\n",
    "print(f\"   - Baseline ROC-AUC: {baseline_eval['roc_auc']:.4f}\")\n",
    "print(f\"   - Improved ROC-AUC: {improved_eval['roc_auc']:.4f}\")\n",
    "print(f\"   - Feature engineering improvement: +{improved_eval['roc_auc'] - baseline_eval['roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n3. KEY BEST PRACTICES:\")\n",
    "best_practices = [\n",
    "    \"Start Simple: Use defaults before customizing\",\n",
    "    \"Provide Enough Time: More time = better models\",\n",
    "    \"Trust the Ensemble: AutoGluon's ensembles usually win\",\n",
    "    \"Feature Engineering: Domain features often help most\",\n",
    "    \"Pipeline Consistency: Same preprocessing in train & inference\",\n",
    "    \"Monitor Continuously: Model drift is inevitable\"\n",
    "]\n",
    "for i, practice in enumerate(best_practices, 1):\n",
    "    print(f\"   {i}. {practice}\")\n",
    "\n",
    "print(\"\\n4. FILES CREATED:\")\n",
    "print(\"   - ./ag_models/figure_7_6_feature_importance.png\")\n",
    "print(\"   - ./ag_models/preprocessing_stats.json\")\n",
    "print(\"   - Various model directories\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Congratulations on completing Chapter 7!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You've Accomplished\n",
    "\n",
    "You have now learned how to:\n",
    "\n",
    "- Set up and use `TabularPredictor` for classification and regression tasks\n",
    "- Leverage AutoGluon's automatic data processing\n",
    "- Customize models, hyperparameters, and ensembles\n",
    "- Build and evaluate an end-to-end ML project (Titanic)\n",
    "- Interpret model behavior with feature importance and SHAP\n",
    "- Ensure data pipeline consistency between training and inference\n",
    "- Monitor models in production for drift\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Apply to Your Own Data**: Try `TabularPredictor` on your own CSV files\n",
    "2. **Experiment with Presets**: Compare `presets=['medium_quality', 'high_quality', 'best_quality']`\n",
    "3. **Deploy a Model**: Use the `predict_passenger_survival` function as a template for a Flask API\n",
    "4. **Set Up Monitoring**: Implement drift detection for production models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}