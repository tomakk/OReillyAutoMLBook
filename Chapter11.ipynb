{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: AutoML in Production - MLOps Integration\n",
    "\n",
    "This notebook accompanies Chapter 11 of the O'Reilly AutoML book. It demonstrates practical MLOps integration patterns for AutoML systems, including:\n",
    "\n",
    "1. MLflow experiment tracking with AutoGluon\n",
    "2. Hierarchical (parent/child) experiment organization\n",
    "3. Model registry and versioning\n",
    "4. Kubeflow pipeline components\n",
    "5. Model monitoring and drift detection\n",
    "6. Automated retraining workflows\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python 3.10+\n",
    "- AutoGluon\n",
    "- MLflow\n",
    "- scikit-learn\n",
    "- pandas, numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install autogluon mlflow scikit-learn pandas numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport time\nfrom datetime import datetime\nimport json\nimport os\n\n# MLflow\nimport mlflow\nfrom mlflow.tracking import MlflowClient\n\n# AutoGluon\nfrom autogluon.tabular import TabularPredictor\n\n# Scikit-learn utilities\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint(f\"MLflow version: {mlflow.__version__}\")\nprint(f\"AutoGluon ready for MLOps integration\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.2: Load Sample Dataset\n",
    "\n",
    "We'll use the California Housing dataset to demonstrate MLOps patterns. This is a regression task suitable for demonstrating model tracking, versioning, and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "\n",
    "# Rename target column\n",
    "df = df.rename(columns={'MedHouseVal': 'target'})\n",
    "\n",
    "# Split data: train/validation/test\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"\\nFeatures: {list(df.columns[:-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.3: MLflow Experiment Tracking with AutoGluon\n",
    "\n",
    "### Snippet 11-1: Basic MLflow Integration\n",
    "\n",
    "This demonstrates the fundamental pattern for tracking AutoGluon experiments with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Snippet 11-1: Basic MLflow Integration with AutoGluon\n\n# Set up MLflow experiment\nmlflow.set_experiment(\"automl-housing-experiment\")\n\nwith mlflow.start_run(run_name=\"autogluon-baseline\") as run:\n    # Log training parameters\n    mlflow.log_param(\"time_limit\", 120)\n    mlflow.log_param(\"eval_metric\", \"root_mean_squared_error\")\n    mlflow.log_param(\"presets\", \"medium_quality\")\n    \n    # Train AutoGluon model\n    predictor = TabularPredictor(\n        label='target',\n        eval_metric='root_mean_squared_error',\n        path='./ag_models_basic'\n    ).fit(\n        train_data=train_df,\n        time_limit=120,\n        presets='medium_quality'\n    )\n    \n    # Get predictions and calculate metrics\n    val_predictions = predictor.predict(val_df.drop(columns=['target']))\n    rmse = np.sqrt(mean_squared_error(val_df['target'], val_predictions))\n    r2 = r2_score(val_df['target'], val_predictions)\n    \n    # Log metrics\n    mlflow.log_metric(\"val_rmse\", rmse)\n    mlflow.log_metric(\"val_r2\", r2)\n    \n    # Log model leaderboard\n    leaderboard = predictor.leaderboard(silent=True)\n    leaderboard.to_csv(\"leaderboard.csv\", index=False)\n    mlflow.log_artifact(\"leaderboard.csv\")\n    \n    print(f\"Run ID: {run.info.run_id}\")\n    print(f\"Validation RMSE: {rmse:.4f}\")\n    print(f\"Validation RÂ²: {r2:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.4: Hierarchical Experiment Tracking\n",
    "\n",
    "### Snippet 11-2: Nested MLflow Runs\n",
    "\n",
    "AutoML generates many models. Using parent/child runs provides clear organization where the parent run captures overall experiment metadata while child runs track individual model details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet 11-2: Hierarchical MLflow Tracking for AutoML\n",
    "\n",
    "def train_with_nested_tracking(train_df, val_df, experiment_name, time_limit=180):\n",
    "    \"\"\"Train AutoGluon with hierarchical MLflow tracking.\"\"\"\n",
    "    \n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    # Parent run captures overall experiment\n",
    "    with mlflow.start_run(run_name=\"automl-parent\") as parent_run:\n",
    "        # Log parent-level parameters\n",
    "        mlflow.log_param(\"framework\", \"autogluon\")\n",
    "        mlflow.log_param(\"time_limit\", time_limit)\n",
    "        mlflow.log_param(\"n_train_samples\", len(train_df))\n",
    "        mlflow.log_param(\"n_features\", len(train_df.columns) - 1)\n",
    "        \n",
    "        # Train AutoGluon\n",
    "        predictor = TabularPredictor(\n",
    "            label='target',\n",
    "            eval_metric='root_mean_squared_error',\n",
    "            path='./ag_models_nested'\n",
    "        ).fit(\n",
    "            train_data=train_df,\n",
    "            time_limit=time_limit,\n",
    "            presets='best_quality'\n",
    "        )\n",
    "        \n",
    "        # Get leaderboard\n",
    "        leaderboard = predictor.leaderboard(silent=True)\n",
    "        \n",
    "        # Create child runs for each model\n",
    "        for idx, row in leaderboard.iterrows():\n",
    "            model_name = row['model']\n",
    "            \n",
    "            with mlflow.start_run(run_name=f\"model-{model_name}\", nested=True):\n",
    "                mlflow.log_param(\"model_type\", model_name)\n",
    "                mlflow.log_metric(\"score_val\", row['score_val'])\n",
    "                mlflow.log_metric(\"pred_time_val\", row['pred_time_val'])\n",
    "                mlflow.log_metric(\"fit_time\", row['fit_time'])\n",
    "                \n",
    "                # Calculate inference latency per sample\n",
    "                latency_per_sample = row['pred_time_val'] / len(val_df) * 1000\n",
    "                mlflow.log_metric(\"latency_ms_per_sample\", latency_per_sample)\n",
    "        \n",
    "        # Log best model metrics at parent level\n",
    "        best_model = leaderboard.iloc[0]\n",
    "        mlflow.log_metric(\"best_score\", best_model['score_val'])\n",
    "        mlflow.log_param(\"best_model\", best_model['model'])\n",
    "        \n",
    "        # Calculate and log validation metrics\n",
    "        val_predictions = predictor.predict(val_df.drop(columns=['target']))\n",
    "        val_rmse = np.sqrt(mean_squared_error(val_df['target'], val_predictions))\n",
    "        mlflow.log_metric(\"final_val_rmse\", val_rmse)\n",
    "        \n",
    "        print(f\"Parent Run ID: {parent_run.info.run_id}\")\n",
    "        print(f\"Best Model: {best_model['model']}\")\n",
    "        print(f\"Best Score: {best_model['score_val']:.4f}\")\n",
    "        print(f\"Models trained: {len(leaderboard)}\")\n",
    "        \n",
    "        return predictor, parent_run.info.run_id\n",
    "\n",
    "# Run hierarchical tracking\n",
    "predictor_nested, parent_run_id = train_with_nested_tracking(\n",
    "    train_df, val_df, \n",
    "    experiment_name=\"automl-hierarchical-tracking\",\n",
    "    time_limit=180\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.5: Model Registry and Versioning\n",
    "\n",
    "### Snippet 11-3: Model Registration Strategies\n",
    "\n",
    "The model registry provides lifecycle management for production models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Snippet 11-3: Model Registration Strategy\n\ndef register_automl_model(predictor, run_id, model_name, alias=\"production\"):\n    \"\"\"Register AutoGluon model and assign an alias for lifecycle management.\"\"\"\n    \n    client = MlflowClient()\n    \n    # Log model as MLflow artifact\n    with mlflow.start_run(run_id=run_id):\n        # Save predictor directory to artifacts\n        predictor_path = predictor.path\n        mlflow.log_artifacts(predictor_path, artifact_path=\"autogluon_predictor\")\n        \n        # Get artifact URI\n        artifact_uri = mlflow.get_artifact_uri(\"autogluon_predictor\")\n    \n    # Register model\n    try:\n        # Create registered model if it doesn't exist\n        client.create_registered_model(\n            name=model_name,\n            description=\"AutoGluon predictor for housing price prediction\"\n        )\n    except mlflow.exceptions.RestException:\n        pass  # Model already exists\n    \n    # Create new version\n    model_version = client.create_model_version(\n        name=model_name,\n        source=artifact_uri,\n        run_id=run_id,\n        description=f\"AutoGluon model registered at {datetime.now()}\"\n    )\n    \n    # Assign alias for lifecycle management\n    client.set_registered_model_alias(\n        name=model_name,\n        alias=alias.lower(),\n        version=model_version.version\n    )\n    \n    print(f\"Registered model: {model_name}\")\n    print(f\"Version: {model_version.version}\")\n    print(f\"Alias: {alias}\")\n    \n    return model_version\n\n# Uncomment below to run (requires MLflow tracking server):\n# model_version = register_automl_model(\n#     predictor_nested, \n#     parent_run_id, \n#     \"housing-price-automl\"\n# )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.6: Model Validation Pipeline\n",
    "\n",
    "### Snippet 11-4: Validation Checks Before Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet 11-4: Model Validation Pipeline\n",
    "\n",
    "class ModelValidator:\n",
    "    \"\"\"Comprehensive model validation before deployment.\"\"\"\n",
    "    \n",
    "    def __init__(self, predictor, baseline_metrics=None):\n",
    "        self.predictor = predictor\n",
    "        self.baseline_metrics = baseline_metrics or {}\n",
    "        self.validation_results = {}\n",
    "    \n",
    "    def validate_performance(self, test_df, target_col='target', threshold=0.95):\n",
    "        \"\"\"Check if model meets performance thresholds.\"\"\"\n",
    "        predictions = self.predictor.predict(test_df.drop(columns=[target_col]))\n",
    "        rmse = np.sqrt(mean_squared_error(test_df[target_col], predictions))\n",
    "        r2 = r2_score(test_df[target_col], predictions)\n",
    "        \n",
    "        baseline_r2 = self.baseline_metrics.get('r2', 0.0)\n",
    "        performance_ratio = r2 / baseline_r2 if baseline_r2 > 0 else 1.0\n",
    "        \n",
    "        passed = performance_ratio >= threshold\n",
    "        \n",
    "        self.validation_results['performance'] = {\n",
    "            'passed': passed,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'baseline_r2': baseline_r2,\n",
    "            'ratio': performance_ratio\n",
    "        }\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def validate_latency(self, test_df, max_latency_ms=100):\n",
    "        \"\"\"Check inference latency requirements.\"\"\"\n",
    "        import time\n",
    "        \n",
    "        # Warm-up\n",
    "        _ = self.predictor.predict(test_df.head(10).drop(columns=['target']))\n",
    "        \n",
    "        # Measure latency\n",
    "        start = time.time()\n",
    "        _ = self.predictor.predict(test_df.drop(columns=['target']))\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        latency_per_sample = (elapsed / len(test_df)) * 1000  # ms\n",
    "        passed = latency_per_sample <= max_latency_ms\n",
    "        \n",
    "        self.validation_results['latency'] = {\n",
    "            'passed': passed,\n",
    "            'latency_ms': latency_per_sample,\n",
    "            'max_allowed_ms': max_latency_ms\n",
    "        }\n",
    "        \n",
    "        return passed\n",
    "    \n",
    "    def validate_all(self, test_df):\n",
    "        \"\"\"Run all validation checks.\"\"\"\n",
    "        perf_passed = self.validate_performance(test_df)\n",
    "        latency_passed = self.validate_latency(test_df)\n",
    "        \n",
    "        all_passed = perf_passed and latency_passed\n",
    "        \n",
    "        return {\n",
    "            'all_passed': all_passed,\n",
    "            'results': self.validation_results\n",
    "        }\n",
    "\n",
    "# Run validation\n",
    "validator = ModelValidator(\n",
    "    predictor_nested, \n",
    "    baseline_metrics={'r2': 0.80}  # Baseline from previous model\n",
    ")\n",
    "\n",
    "validation_report = validator.validate_all(test_df)\n",
    "print(json.dumps(validation_report, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.7: Model Monitoring and Drift Detection\n",
    "\n",
    "### Snippet 11-5: Data Drift Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet 11-5: Data Drift Detection\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "class DriftDetector:\n",
    "    \"\"\"Detect data and prediction drift in AutoML systems.\"\"\"\n",
    "    \n",
    "    def __init__(self, reference_data, feature_columns):\n",
    "        self.reference_data = reference_data\n",
    "        self.feature_columns = feature_columns\n",
    "        self.reference_stats = self._compute_stats(reference_data)\n",
    "    \n",
    "    def _compute_stats(self, data):\n",
    "        \"\"\"Compute reference statistics.\"\"\"\n",
    "        stats_dict = {}\n",
    "        for col in self.feature_columns:\n",
    "            stats_dict[col] = {\n",
    "                'mean': data[col].mean(),\n",
    "                'std': data[col].std(),\n",
    "                'min': data[col].min(),\n",
    "                'max': data[col].max(),\n",
    "                'quantiles': data[col].quantile([0.25, 0.5, 0.75]).values\n",
    "            }\n",
    "        return stats_dict\n",
    "    \n",
    "    def calculate_psi(self, reference, current, bins=10):\n",
    "        \"\"\"Calculate Population Stability Index.\"\"\"\n",
    "        # Create bins from reference data\n",
    "        min_val = min(reference.min(), current.min())\n",
    "        max_val = max(reference.max(), current.max())\n",
    "        bins_edges = np.linspace(min_val, max_val, bins + 1)\n",
    "        \n",
    "        ref_counts, _ = np.histogram(reference, bins=bins_edges)\n",
    "        curr_counts, _ = np.histogram(current, bins=bins_edges)\n",
    "        \n",
    "        # Add small value to avoid division by zero\n",
    "        ref_pct = (ref_counts + 1) / (len(reference) + bins)\n",
    "        curr_pct = (curr_counts + 1) / (len(current) + bins)\n",
    "        \n",
    "        psi = np.sum((curr_pct - ref_pct) * np.log(curr_pct / ref_pct))\n",
    "        return psi\n",
    "    \n",
    "    def detect_drift(self, current_data, psi_threshold=0.2):\n",
    "        \"\"\"Detect drift in current data vs reference.\"\"\"\n",
    "        drift_report = {\n",
    "            'drift_detected': False,\n",
    "            'features_with_drift': [],\n",
    "            'feature_psi': {}\n",
    "        }\n",
    "        \n",
    "        for col in self.feature_columns:\n",
    "            psi = self.calculate_psi(\n",
    "                self.reference_data[col],\n",
    "                current_data[col]\n",
    "            )\n",
    "            drift_report['feature_psi'][col] = psi\n",
    "            \n",
    "            if psi > psi_threshold:\n",
    "                drift_report['drift_detected'] = True\n",
    "                drift_report['features_with_drift'].append({\n",
    "                    'feature': col,\n",
    "                    'psi': psi,\n",
    "                    'severity': 'high' if psi > 0.25 else 'medium'\n",
    "                })\n",
    "        \n",
    "        return drift_report\n",
    "\n",
    "# Initialize drift detector with training data\n",
    "feature_cols = [c for c in train_df.columns if c != 'target']\n",
    "drift_detector = DriftDetector(train_df, feature_cols)\n",
    "\n",
    "# Check for drift in test data (simulating production data)\n",
    "drift_report = drift_detector.detect_drift(test_df)\n",
    "print(\"Drift Detection Report:\")\n",
    "print(json.dumps(drift_report, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.8: Regression Testing\n",
    "\n",
    "### Snippet 11-6: Model Regression Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet 11-6: Model Regression Testing\n",
    "\n",
    "class ModelRegressionTester:\n",
    "    \"\"\"Test new models against production baselines.\"\"\"\n",
    "    \n",
    "    def __init__(self, test_cases):\n",
    "        self.test_cases = test_cases  # DataFrame with inputs and expected outputs\n",
    "        \n",
    "    def run_regression_tests(self, new_predictor, old_predictor, \n",
    "                            tolerance=0.1, max_regression_pct=0.05):\n",
    "        \"\"\"Compare new model against old model on test cases.\"\"\"\n",
    "        \n",
    "        features = self.test_cases.drop(columns=['target'])\n",
    "        expected = self.test_cases['target']\n",
    "        \n",
    "        old_predictions = old_predictor.predict(features)\n",
    "        new_predictions = new_predictor.predict(features)\n",
    "        \n",
    "        # Calculate metrics for both\n",
    "        old_rmse = np.sqrt(mean_squared_error(expected, old_predictions))\n",
    "        new_rmse = np.sqrt(mean_squared_error(expected, new_predictions))\n",
    "        \n",
    "        # Check for regression\n",
    "        regression_pct = (new_rmse - old_rmse) / old_rmse if old_rmse > 0 else 0\n",
    "        \n",
    "        # Count significant prediction differences\n",
    "        diff = np.abs(new_predictions - old_predictions)\n",
    "        significant_changes = (diff > tolerance * np.abs(old_predictions)).sum()\n",
    "        \n",
    "        results = {\n",
    "            'passed': regression_pct <= max_regression_pct,\n",
    "            'old_rmse': old_rmse,\n",
    "            'new_rmse': new_rmse,\n",
    "            'regression_pct': regression_pct,\n",
    "            'significant_prediction_changes': significant_changes,\n",
    "            'total_test_cases': len(self.test_cases)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create regression test suite using a sample of test data\n",
    "regression_tester = ModelRegressionTester(test_df.head(100))\n",
    "\n",
    "# Note: In practice, you'd compare new model vs deployed production model\n",
    "# Here we compare the same model as a demonstration\n",
    "regression_results = regression_tester.run_regression_tests(\n",
    "    predictor_nested, \n",
    "    predictor_nested  # Would be different in production\n",
    ")\n",
    "\n",
    "print(\"Regression Test Results:\")\n",
    "print(json.dumps(regression_results, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.9: Automated Retraining Workflow\n",
    "\n",
    "### Snippet 11-7: Retraining Trigger Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet 11-7: Automated Retraining Trigger\n",
    "\n",
    "class RetrainingOrchestrator:\n",
    "    \"\"\"Orchestrate automated model retraining based on triggers.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.drift_threshold = config.get('drift_psi_threshold', 0.2)\n",
    "        self.performance_threshold = config.get('performance_threshold', 0.95)\n",
    "        \n",
    "    def evaluate_triggers(self, drift_report, performance_metrics):\n",
    "        \"\"\"Determine if retraining should be triggered.\"\"\"\n",
    "        triggers = []\n",
    "        \n",
    "        # Check drift trigger\n",
    "        if drift_report.get('drift_detected', False):\n",
    "            triggers.append({\n",
    "                'type': 'data_drift',\n",
    "                'reason': f\"Drift detected in {len(drift_report.get('features_with_drift', []))} features\",\n",
    "                'severity': 'high'\n",
    "            })\n",
    "        \n",
    "        # Check performance degradation\n",
    "        current_perf = performance_metrics.get('current_r2', 1.0)\n",
    "        baseline_perf = performance_metrics.get('baseline_r2', 1.0)\n",
    "        perf_ratio = current_perf / baseline_perf if baseline_perf > 0 else 1.0\n",
    "        \n",
    "        if perf_ratio < self.performance_threshold:\n",
    "            triggers.append({\n",
    "                'type': 'performance_degradation',\n",
    "                'reason': f\"Performance dropped to {perf_ratio:.2%} of baseline\",\n",
    "                'severity': 'high' if perf_ratio < 0.9 else 'medium'\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'should_retrain': len(triggers) > 0,\n",
    "            'triggers': triggers,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def execute_retraining(self, train_df, retrain_config):\n",
    "        \"\"\"Execute retraining pipeline.\"\"\"\n",
    "        print(f\"Starting retraining at {datetime.now()}\")\n",
    "        \n",
    "        mlflow.set_experiment(\"automl-retraining\")\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"retrain-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"):\n",
    "            mlflow.log_param(\"retrain_reason\", retrain_config.get('reason', 'scheduled'))\n",
    "            mlflow.log_param(\"time_limit\", retrain_config.get('time_limit', 300))\n",
    "            \n",
    "            predictor = TabularPredictor(\n",
    "                label='target',\n",
    "                eval_metric='root_mean_squared_error',\n",
    "                path=f'./ag_models_retrain_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "            ).fit(\n",
    "                train_data=train_df,\n",
    "                time_limit=retrain_config.get('time_limit', 300),\n",
    "                presets=retrain_config.get('presets', 'best_quality')\n",
    "            )\n",
    "            \n",
    "            return predictor\n",
    "\n",
    "# Initialize orchestrator\n",
    "orchestrator = RetrainingOrchestrator({\n",
    "    'drift_psi_threshold': 0.2,\n",
    "    'performance_threshold': 0.95\n",
    "})\n",
    "\n",
    "# Evaluate triggers\n",
    "trigger_result = orchestrator.evaluate_triggers(\n",
    "    drift_report=drift_report,\n",
    "    performance_metrics={'current_r2': 0.85, 'baseline_r2': 0.90}\n",
    ")\n",
    "\n",
    "print(\"Retraining Trigger Evaluation:\")\n",
    "print(json.dumps(trigger_result, indent=2))\n",
    "\n",
    "# Execute retraining if triggered (commented out to save time)\n",
    "# if trigger_result['should_retrain']:\n",
    "#     new_predictor = orchestrator.execute_retraining(\n",
    "#         train_df,\n",
    "#         {'time_limit': 180, 'reason': trigger_result['triggers'][0]['type']}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.10: Kubeflow Pipeline Component Example\n",
    "\n",
    "This cell shows how to define a Kubeflow pipeline component for AutoML training. Note: This requires `kfp` package and Kubeflow cluster to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kubeflow Pipeline Component Definition (reference implementation)\n",
    "\n",
    "AUTOML_COMPONENT_YAML = \"\"\"\n",
    "name: AutoGluon Training Component\n",
    "description: Train AutoGluon model with MLflow tracking\n",
    "\n",
    "inputs:\n",
    "  - name: train_data_path\n",
    "    type: String\n",
    "    description: Path to training data\n",
    "  - name: target_column\n",
    "    type: String\n",
    "    description: Name of target column\n",
    "  - name: time_limit\n",
    "    type: Integer\n",
    "    default: 300\n",
    "  - name: presets\n",
    "    type: String\n",
    "    default: best_quality\n",
    "  - name: mlflow_tracking_uri\n",
    "    type: String\n",
    "\n",
    "outputs:\n",
    "  - name: model_path\n",
    "    type: String\n",
    "  - name: metrics\n",
    "    type: JsonObject\n",
    "\n",
    "implementation:\n",
    "  container:\n",
    "    image: autogluon-mlops:latest\n",
    "    command:\n",
    "      - python\n",
    "      - train_component.py\n",
    "    args:\n",
    "      - --train-data={{inputs.train_data_path}}\n",
    "      - --target-column={{inputs.target_column}}\n",
    "      - --time-limit={{inputs.time_limit}}\n",
    "      - --presets={{inputs.presets}}\n",
    "      - --mlflow-uri={{inputs.mlflow_tracking_uri}}\n",
    "      - --output-path={{outputs.model_path}}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Kubeflow Component Definition:\")\n",
    "print(AUTOML_COMPONENT_YAML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.11: Summary\n",
    "\n",
    "This notebook demonstrated key MLOps patterns for AutoML systems:\n",
    "\n",
    "1. **MLflow Integration** - Tracking experiments, parameters, and metrics\n",
    "2. **Hierarchical Tracking** - Parent/child runs for multi-model experiments\n",
    "3. **Model Registry** - Version management and staging\n",
    "4. **Validation Pipeline** - Performance and latency checks\n",
    "5. **Drift Detection** - PSI-based data drift monitoring\n",
    "6. **Regression Testing** - Ensuring model quality over time\n",
    "7. **Automated Retraining** - Trigger-based model updates\n",
    "\n",
    "These patterns enable reliable, reproducible AutoML deployments in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary files (optional)\n",
    "import shutil\n",
    "\n",
    "# Uncomment to clean up:\n",
    "# for path in ['./ag_models_basic', './ag_models_nested']:\n",
    "#     if os.path.exists(path):\n",
    "#         shutil.rmtree(path)\n",
    "#         print(f\"Removed {path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl-fresh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}